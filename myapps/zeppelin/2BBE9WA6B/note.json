{
  "paragraphs": [
    {
      "text": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature",
      "dateUpdated": "Jan 17, 2016 12:05:24 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772954_-1308225136",
      "id": "20160110-231612_1600912219",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.twitter.algebird.MinHasher\nimport com.twitter.algebird.MinHasher32\nimport com.twitter.algebird.MinHashSignature\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:24 AM",
      "dateFinished": "Jan 17, 2016 12:05:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load dataset including tags",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")",
      "dateUpdated": "Jan 17, 2016 12:05:24 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772954_-1308225136",
      "id": "20160110-231612_487924464",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [category: string, description: string, id: bigint, img: string, tags: array\u003cstring\u003e, title: string]\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:28 AM",
      "dateFinished": "Jan 17, 2016 12:05:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Number Of Distinct Tags Found In The Dataset",
      "text": "val distinctCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").distinct()\ndistinctCounts.count()",
      "dateUpdated": "Jan 17, 2016 12:05:24 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772954_-1308225136",
      "id": "20160110-231612_249580154",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "distinctCounts: org.apache.spark.sql.DataFrame \u003d [tag: string]\nres289: Long \u003d 108\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:32 AM",
      "dateFinished": "Jan 17, 2016 12:05:34 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution Of Tags Within Dataset",
      "text": "val tagCounts \u003d itemsDF.select(explode($\"tags\") as \"tag\").groupBy($\"tag\")\n  .agg(count($\"tag\").as(\"count\"))\n  .orderBy($\"count\".desc)\n  .limit(10)\n\nz.show(tagCounts)",
      "dateUpdated": "Jan 17, 2016 12:05:24 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "tag",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772954_-1308225136",
      "id": "20160110-231612_848706349",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "tag\tcount\nJVM\t31\nJava\t31\nSQL\t24\nHadoop\t16\nLibrary\t15\nDatabase\t12\nData Procesing Execution Engine\t11\nPython\t11\nSpark\t9\nUI\t8\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:33 AM",
      "dateFinished": "Jan 17, 2016 12:05:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert Row to Item",
      "text": "import org.apache.spark.sql.Row\n\n// Convert from RDD[Row] to RDD[Item]\nval itemsRDD \u003d itemsDF.select($\"id\", $\"title\", $\"description\", $\"img\", $\"tags\").map(row \u003d\u003e\n  Item(row.getLong(0), row.getString(1), row.getString(2), row.getString(3), row.getSeq[String](4).toSet)\n)",
      "dateUpdated": "Jan 17, 2016 12:05:24 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772954_-1308225136",
      "id": "20160110-231612_101779729",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nitemsRDD: org.apache.spark.rdd.RDD[Item] \u003d MapPartitionsRDD[2698] at map at \u003cconsole\u003e:124\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:35 AM",
      "dateFinished": "Jan 17, 2016 12:05:36 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Approx Jaccard Similarity between all Distinct item pairs",
      "text": "val numHashes \u003d 100\nval minApproxJaccardSimilarityThreshold \u003d 0.1\n\nval numBands \u003d MinHasher.pickBands(minApproxJaccardSimilarityThreshold, numHashes)\nval minHasher \u003d new MinHasher32(numHashes, numBands)\n\nval allItemPairsRDD \u003d itemsRDD.cartesian(itemsRDD)\n\n// Filter out duplicates and preserve only the pairs with left id \u003c right id\n//val approxDistinctItemPairsRDD \u003d allItemPairsRDD.filter(itemPair \u003d\u003e itemPair._1.id \u003c itemPair._2.id)\n//approxDistinctItemPairsRDD.count()\n\n// Calculate Jaccard Similarity between all item pairs (cartesian, then de-duped)\n// Only keep pairs with a Jaccard Similarity above a specific threshold\nval approxSimilarItemsAboveThresholdRDD \u003d allItemPairsRDD.flatMap(itemPair \u003d\u003e {\n  val approxJaccardSim \u003d getApproxLSHJaccardSimilarity(minHasher, itemPair._1, itemPair._2)\n  if (approxJaccardSim \u003e\u003d minApproxJaccardSimilarityThreshold)\n    Some(itemPair._1.id, itemPair._2.id, approxJaccardSim)\n  else\n    None\n})\n\nval approxSimilarItemPairCount \u003d approxSimilarItemsAboveThresholdRDD.count()\napproxSimilarItemsAboveThresholdRDD.collect().mkString(\",\")",
      "dateUpdated": "Jan 17, 2016 12:05:24 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772954_-1308225136",
      "id": "20160110-231612_165821720",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "numHashes: Int \u003d 100\nminApproxJaccardSimilarityThreshold: Double \u003d 0.1\nnumBands: Int \u003d 57\nminHasher: com.twitter.algebird.MinHasher32 \u003d com.twitter.algebird.MinHasher32@13296343\nallItemPairsRDD: org.apache.spark.rdd.RDD[(Item, Item)] \u003d CartesianRDD[2699] at cartesian at \u003cconsole\u003e:125\napproxSimilarItemsAboveThresholdRDD: org.apache.spark.rdd.RDD[(Long, Long, Double)] \u003d MapPartitionsRDD[2700] at flatMap at \u003cconsole\u003e:143\napproxSimilarItemPairCount: Long \u003d 1926\nres307: String \u003d (1,1,1.0),(1,6,0.16),(1,7,0.14),(1,8,0.3),(1,9,0.28),(1,10,0.31),(1,11,0.29),(1,12,0.66),(1,13,0.29),(1,14,0.24),(1,16,0.17),(1,18,0.23),(1,19,0.28),(1,20,0.23),(1,21,0.2),(1,22,0.29),(1,25,0.22),(1,27,0.2),(1,28,0.16),(1,29,0.27),(1,30,0.18),(1,31,0.35),(1,32,0.32),(1,33,0.27),(1,34,0.24),(1,37,0.1),(1,38,0.2),(1,42,0.14),(1,43,0.13),(1,45,0.24),(1,54,0.28),(1,59,0.24),(1,60,0.24),(1,68,0.28),(1,70,0.21),(1,73,0.14),(1,74,0.3),(1,75,0.24),(1,77,0.13),(1,79,0.5),(1,80,0.11),(1,81,0.11),(2,2,1.0),(2,51,0.13),(2,73,0.12),(3,3,1.0),(3,20,0.11),(3,21,0.11),(4,4,1.0),(4,24,0.14),(5,5,1.0),(5,49,0.47),(5,50,0.38),(5,55,0.39),(5,70,0.13),(6,1,0.16),(6,6,1.0),(6,7,0.79),(6,8,0.17),(6,9,0.24),(6,10,0.17),(6,11,0.16),(6,12,0.17),(6,13,0.23),(6,14,0.27),(6,16,0.2),(6,17,0.17),(6,1..."
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:35 AM",
      "dateFinished": "Jan 17, 2016 12:05:40 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_112358564",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:36 AM",
      "dateFinished": "Jan 17, 2016 12:05:40 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Similarity Pathway",
      "text": "val approxSimilarItemsAboveThresholdEdgsRDD \u003d approxSimilarItemsAboveThresholdRDD.map(rdd \u003d\u003e {\n  Edge(rdd._1, rdd._2, rdd._3) \n})\n\nval approxGraph \u003d Graph.fromEdges(approxSimilarItemsAboveThresholdEdgsRDD, 0L)",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_149269730",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "approxSimilarItemsAboveThresholdEdgsRDD: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] \u003d MapPartitionsRDD[2701] at map at \u003cconsole\u003e:138\napproxGraph: org.apache.spark.graphx.Graph[Long,Double] \u003d org.apache.spark.graphx.impl.GraphImpl@742175fb\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:41 AM",
      "dateFinished": "Jan 17, 2016 12:05:42 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Example 1\n//\n// Shortest Path\n// 21 (ElasticSearch) -0.15-\u003e 28 (ZooKeeper) -0.1-\u003e 56 (MicroStrategy)\n\n// Tag Analysis of Shortest Path\n//\n// 21 Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations\n// 28 Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy\n// 56 BI, UI, Visualization, SQL\n\nval src \u003d 21 // ElasticSearch\nval dest \u003d 56 // MicroStrategy\n\nval approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval approxShortestPathFromSrcToDest \u003d approxShortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_1932601957",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "src: Int \u003d 21\ndest: Int \u003d 56\n\u003cconsole\u003e:145: error: not found: value dijkstra\n       val approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n                                     ^\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:42 AM",
      "dateFinished": "Jan 17, 2016 12:05:43 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Example 2\n//\n// Shortest Path\n// 35 (NLTK) -\u003e 37 (iPython/Jupyter) -\u003e 41 (SQL) -\u003e 42 (Scala)\n//\n// Tag Analysis of Shortest Path\n//\n// 35:  Library, NLP, Python, Text Analytics\n// 9:   Library, Java, JVM, Graph Analytics, Batch (Apache Giraph)\n// 42:  Programming Language, Functional, JVM, Static Typing, Compiled\n\n// 37:  Notebook, Python, Java, Scala, R, Visualization, SQL\n// 41:  Programming Language, SQL, RDBMS, Interpreted\n\nval src \u003d 35 // ElasticSearch\nval dest \u003d 42 // MicroStrategy\n\nval approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n\n// Filter out only the ones with dest as the destination vertex\nval approxShortestPathFromSrcToDest \u003d approxShortestPathGraph.vertices.filter(_._1 \u003d\u003d dest).map(_._2).collect()(0)._2",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_1805894630",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "src: Int \u003d 35\ndest: Int \u003d 42\n\u003cconsole\u003e:145: error: not found: value dijkstra\n       val approxShortestPathGraph \u003d dijkstra(approxGraph, src)\n                                     ^\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:43 AM",
      "dateFinished": "Jan 17, 2016 12:05:43 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Power Iteration Clustering Of Items Based On Approx LSH Tag Jaccard Similarity",
      "text": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\n\nval clustering \u003d new PowerIterationClustering().setK(5).setMaxIterations(10)\n\nval clusteringModel \u003d clustering.run(approxSimilarItemsAboveThresholdRDD)\n\nval clusterAssignmentsRDD \u003d clusteringModel.assignments.map { assignment \u003d\u003e\n  (assignment.id, assignment.cluster)\n}",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_2116128098",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}\nclustering: org.apache.spark.mllib.clustering.PowerIterationClustering \u003d org.apache.spark.mllib.clustering.PowerIterationClustering@465b1737\nclusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel \u003d org.apache.spark.mllib.clustering.PowerIterationClusteringModel@f307b5d\nclusterAssignmentsRDD: org.apache.spark.rdd.RDD[(Long, Int)] \u003d MapPartitionsRDD[2842] at map at \u003cconsole\u003e:144\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:43 AM",
      "dateFinished": "Jan 17, 2016 12:05:49 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Convert The ClusterAssignmentsRDD Into A DataFrame",
      "text": "val schema \u003d StructType(StructField(\"itemId\", LongType, true) :: StructField(\"clusterId\", IntegerType, true) :: Nil)\n\nval clusterAssignmentsRowRDD \u003d clusterAssignmentsRDD.map(clusterAssignmentRDD \u003d\u003e \n  Row(clusterAssignmentRDD._1, clusterAssignmentRDD._2))\n\nval clusterAssignmentsDF \u003d sqlContext.createDataFrame(clusterAssignmentsRowRDD, schema)",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_1535552760",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "schema: org.apache.spark.sql.types.StructType \u003d StructType(StructField(itemId,LongType,true), StructField(clusterId,IntegerType,true))\nclusterAssignmentsRowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[2843] at map at \u003cconsole\u003e:146\nclusterAssignmentsDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int]\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:44 AM",
      "dateFinished": "Jan 17, 2016 12:05:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Distribution of items within a cluster",
      "text": "val joinedItemsClustersCountDF \u003d clusterAssignmentsDF.select($\"itemId\", $\"clusterId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").groupBy($\"clusterId\", $\"tags\")\n  .agg(count($\"itemId\")).sort($\"clusterId\" desc)\n\nz.show(joinedItemsClustersCountDF)",
      "dateUpdated": "Jan 17, 2016 12:05:28 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count(itemId)",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "clusterId",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_564058095",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "clusterId\ttags\tcount(itemId)\n4\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\t1\n4\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\t1\n4\tWrappedArray(Library, Streaming, AWS)\t1\n4\tWrappedArray(Library, Python, Machine Learning)\t1\n4\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\t1\n4\tWrappedArray(Library, Graph Analytics, Spark)\t1\n4\tWrappedArray(BI, UI, Visualization, SQL)\t2\n4\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\t1\n4\tWrappedArray(Database, Data Warehouse, SQL)\t2\n4\tWrappedArray(Library, Spark, HiveQL, SQL)\t1\n4\tWrappedArray(Library, Spark, Streaming)\t1\n4\tWrappedArray(Library, Deep Learning, Neural Networks)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\t1\n4\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\t1\n4\tWrappedArray(Library, Spark, Machine Learning)\t1\n4\tWrappedArray(Database, SQL, RDBMS, Transactional)\t3\n4\tWrappedArray(Cluster Resource Manager, Docker, Container)\t1\n4\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\t1\n3\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\t1\n3\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\t1\n3\tWrappedArray(Library, Machine Learning, Java, JVM)\t1\n3\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\t1\n3\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\t1\n3\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\t1\n3\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\t1\n3\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\t1\n3\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\t1\n3\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\t1\n3\tWrappedArray(Library, NLP, Python, Text Analytics)\t1\n2\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\t1\n2\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\t4\n2\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\t1\n2\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\t1\n2\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\t1\n2\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\t1\n1\tWrappedArray(Cloud Provider, AWS)\t1\n1\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\t2\n1\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\t1\n1\tWrappedArray(Container, Linux, DevOps, Deployment)\t1\n1\tWrappedArray(Cloud Provider, Google)\t1\n1\tWrappedArray(Cloud Provider, Microsoft)\t1\n1\tWrappedArray(File Format, Key Value Store)\t2\n1\tWrappedArray(File Format)\t1\n1\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\t1\n1\tWrappedArray(File Format, Evolving Schema, Nested Schema)\t1\n1\tWrappedArray(Cloud Provider, Data Center)\t1\n0\tWrappedArray(Library, Java, JVM, Log Collection)\t1\n0\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\t1\n0\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\t1\n0\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\t1\n0\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n0\tWrappedArray(Library, Search, Java, JVM, Python)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\t1\n0\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\t1\n0\tWrappedArray(Library, Graph Analytics, Java, JVM)\t1\n0\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\t1\n0\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\t1\n0\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\t1\n0\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\t1\n0\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\t1\n0\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\t1\n0\tWrappedArray(Data Import, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\t1\n0\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\t1\n0\tWrappedArray(File System, Hadoop, Java, JVM)\t1\n0\tWrappedArray(Streaming, Java, JVM)\t1\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:50 AM",
      "dateFinished": "Jan 17, 2016 12:05:55 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cluster Details",
      "text": "// Enrich the cluster assignment tuples with itemsDF\nval joinedItemsClustersDF \u003d clusterAssignmentsDF.select($\"clusterId\", $\"itemId\")\n  .join(itemsDF.select($\"id\", $\"title\", $\"tags\"), $\"itemId\" \u003d\u003d\u003d $\"id\").select($\"itemId\", $\"clusterId\", $\"title\", $\"tags\").sort($\"clusterId\", $\"itemId\")\n  \nz.show(joinedItemsClustersDF)",
      "dateUpdated": "Jan 17, 2016 12:05:31 AM",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 474.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "clusterId",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772955_-1308609885",
      "id": "20160110-231612_1415023817",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "itemId\tclusterId\ttitle\ttags\n6\t0\tApache Flink\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\n7\t0\tApache Spark\tWrappedArray(Data Procesing Execution Engine, Java, Scala, JVM, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\n8\t0\tApache Flume\tWrappedArray(Library, Java, JVM, Log Collection)\n9\t0\tApache Giraph\tWrappedArray(Library, Java, JVM, Graph Analytics, Batch)\n10\t0\tApache HDFS\tWrappedArray(File System, Hadoop, Java, JVM)\n11\t0\tApache YARN\tWrappedArray(Cluster Resource Manager, Hadoop, Java, JVM)\n12\t0\tApache HBase\tWrappedArray(Database, Hadoop, NoSQL, Java, JVM, Eventually Consistent)\n13\t0\tApache MapReduce\tWrappedArray(Data Procesing Execution Engine, Hadoop, Java, JVM, Python)\n16\t0\tApache HUE\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java, JVM)\n18\t0\tApache Kafka\tWrappedArray(Message Broker, Java, JVM, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\n19\t0\tApache Lucene\tWrappedArray(Library, Search, Java, JVM, Python)\n20\t0\tApache Solr\tWrappedArray(Search Engine, Java, JVM, REST API, UI, Python, Ruby, XML, JSON)\n21\t0\tElasticSearch\tWrappedArray(Search Engine, Java, JVM, Python, REST API, Lucene, XML, JSON, Aggregations)\n27\t0\tApache Pig\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n28\t0\tApache ZooKeeper\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, Lazy)\n29\t0\tStanford CoreNLP\tWrappedArray(Library, NLP, Java, JVM, Text Analytics)\n30\t0\tApache Tez\tWrappedArray(Data Procesing Execution Engine, Hadoop, YARN, Query Processing, Java, JVM, Lazy, HiveQL, Pig, SQL)\n31\t0\tApache Storm\tWrappedArray(Streaming, Java, JVM)\n32\t0\tApache Sqoop\tWrappedArray(Data Import, Hadoop, Java, JVM)\n33\t0\tApache Oozie\tWrappedArray(Workflow, Hadoop, Java, JVM, UI)\n34\t0\tApache Nifi\tWrappedArray(Workflow, Streaming, Message Broker, Java, JVM, UI)\n38\t0\tApache Zeppelin\tWrappedArray(Notebook, Python, Java, Scala, R, JVM, HiveQL, Cassandra, Visuatlization, SQL)\n74\t0\tNeo4j\tWrappedArray(Library, Graph Analytics, Java, JVM)\n79\t0\tTitan GraphDB\tWrappedArray(Database, Graph, Graph Analytics, Java, JVM, Transactional)\n2\t1\tTachyon\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\n4\t1\tDocker\tWrappedArray(Container, Linux, DevOps, Deployment)\n5\t1\tMicrosft Azure\tWrappedArray(Cloud Provider, Microsoft)\n40\t1\tR\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n44\t1\tPython\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n49\t1\tAmazon Web Services\tWrappedArray(Cloud Provider, AWS)\n50\t1\tGoogle Cloud Platform\tWrappedArray(Cloud Provider, Google)\n51\t1\tRedis\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\n52\t1\tJSON\tWrappedArray(File Format, Key Value Store)\n53\t1\tXML\tWrappedArray(File Format, Key Value Store)\n55\t1\tOn-Premise\tWrappedArray(Cloud Provider, Data Center)\n64\t1\tCSV\tWrappedArray(File Format)\n76\t1\tProtobuffers\tWrappedArray(File Format, Evolving Schema, Nested Schema)\n15\t2\tHortonworks\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n26\t2\tApache ORC\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\n41\t2\tSQL\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\n46\t2\tMapR\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n47\t2\tCloudera\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n48\t2\tIBM BigInsights\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n66\t2\tRedshift\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\n68\t2\tDynamoDB\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\n77\t2\tS3\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\n1\t3\tApache Cassandra\tWrappedArray(Database, NoSQL, Java, JVM, Eventually Consistent, Transactional)\n14\t3\tApache Hive\tWrappedArray(Data Procesing Execution Engine, Hadoop, HiveQL, SQL, Query Processing, Java, JVM, MapReduce)\n22\t3\tApache Mahout\tWrappedArray(Library, Machine Learning, Java, JVM)\n25\t3\tApache Parquet\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, JVM, C++, Python)\n35\t3\tNLTK\tWrappedArray(Library, NLP, Python, Text Analytics)\n37\t3\tiPython/Jupyter\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\n42\t3\tScala\tWrappedArray(Programming Language, Functional, JVM, Static Typing, Compiled)\n43\t3\tJava\tWrappedArray(Programming Language, Object Oriented, JVM, Static Typing, Compiled)\n45\t3\tPresto\tWrappedArray(Data Procesing Execution Engine, Query Processing, Java, JVM, SQL, Machine Learning)\n71\t3\tElastic MapReduce\tWrappedArray(Data Procesing Execution Engine, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\n73\t3\tMemcached\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\n3\t4\tApache Ambari\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\n17\t4\tApache Impala\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, C++, Batch Analytics)\n23\t4\tApache Drill\tWrappedArray(Data Procesing Execution Engine, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\n24\t4\tApache Mesos\tWrappedArray(Cluster Resource Manager, Docker, Container)\n36\t4\tSci-Kit Learn\tWrappedArray(Library, Python, Machine Learning)\n39\t4\tTableau\tWrappedArray(BI, UI, Visualization, SQL)\n54\t4\tMongoDB\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\n56\t4\tMicroStrategy\tWrappedArray(BI, UI, Visualization, SQL)\n57\t4\tKnime\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\n59\t4\tOracle\tWrappedArray(Database, SQL, RDBMS, Transactional)\n60\t4\tMySQL\tWrappedArray(Database, SQL, RDBMS, Transactional)\n61\t4\tSpark ML/MLlib\tWrappedArray(Library, Spark, Machine Learning)\n62\t4\tSpark Streaming\tWrappedArray(Library, Spark, Streaming)\n63\t4\tSpark SQL\tWrappedArray(Library, Spark, HiveQL, SQL)\n65\t4\tDeep Learning 4J\tWrappedArray(Library, Deep Learning, Neural Networks)\n67\t4\tKinesis\tWrappedArray(Library, Streaming, AWS)\n69\t4\tSpark GraphX\tWrappedArray(Library, Graph Analytics, Spark)\n70\t4\tSQL Server\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\n72\t4\tDato GraphLab Create\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\n75\t4\tPostgres\tWrappedArray(Database, SQL, RDBMS, Transactional)\n78\t4\tTensor Flow\tWrappedArray(Data Procesing Execution Engine, Deep Learning, Neural Networks)\n80\t4\tTeradata\tWrappedArray(Database, Data Warehouse, SQL)\n81\t4\tVertica\tWrappedArray(Database, Data Warehouse, SQL)\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:51 AM",
      "dateFinished": "Jan 17, 2016 12:05:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Show the intersection of tags within each cluster\nval clusterTagIntersectionDF \u003d joinedItemsClustersDF.explode(\"tags\", \"tag\"){c: List[String] \u003d\u003e c}",
      "dateUpdated": "Jan 17, 2016 12:05:31 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772956_-1310533629",
      "id": "20160110-231612_1222691061",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "clusterTagIntersectionDF: org.apache.spark.sql.DataFrame \u003d [itemId: bigint, clusterId: int, title: string, tags: array\u003cstring\u003e, tag: string]\n"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:55 AM",
      "dateFinished": "Jan 17, 2016 12:05:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// TODO:  Given an itemId (or List of itemIds), recommend more itemIds based on similar cluster \n//        and/or closest jaccard similarity ",
      "dateUpdated": "Jan 17, 2016 12:05:31 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772956_-1310533629",
      "id": "20160110-231612_529948585",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:56 AM",
      "dateFinished": "Jan 17, 2016 12:05:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jan 17, 2016 12:05:31 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452467772956_-1310533629",
      "id": "20160110-231612_2010102102",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 10, 2016 11:16:12 PM",
      "dateStarted": "Jan 17, 2016 12:05:56 AM",
      "dateFinished": "Jan 17, 2016 12:05:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "TODO: Live Recs/09: Generate Item-to-Item Description-Similarity NLP-based Recs",
  "id": "2BBE9WA6B",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}