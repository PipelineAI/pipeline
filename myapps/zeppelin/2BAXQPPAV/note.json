{
  "paragraphs": [
    {
      "title": "Retrieve dataset",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/datasets/nlp/country-lyrics.json\")\n  .select($\"id\", $\"title\", $\"url\", $\"lyrics\")",
      "dateUpdated": "Mar 10, 2016 12:08:32 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872022_-2076523971",
      "id": "20160129-091432_588185682",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "itemsDF: org.apache.spark.sql.DataFrame \u003d [id: bigint, title: string, url: string, lyrics: string]\n"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Mar 10, 2016 12:08:32 PM",
      "dateFinished": "Mar 10, 2016 12:08:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Tokenize",
      "text": "import org.apache.spark.ml.feature.RegexTokenizer\n\n// Split each document into words\nval tokenizer \u003d new RegexTokenizer()\n  .setInputCol(\"lyrics\")\n  .setOutputCol(\"words\")\n  .setGaps(false)\n  .setPattern(\"\\\\p{L}+\")",
      "dateUpdated": "Mar 10, 2016 12:08:37 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872022_-2076523971",
      "id": "20160129-091432_1220039619",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.RegexTokenizer\ntokenizer: org.apache.spark.ml.feature.RegexTokenizer \u003d regexTok_681cc1754ec0\n"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Mar 10, 2016 12:08:37 PM",
      "dateFinished": "Mar 10, 2016 12:08:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Remove Common Stop words",
      "text": "import org.apache.spark.ml.feature.StopWordsRemover\n\n// Filter out stopwords\n// The following list will be used by default if we don\u0027t specify a list:  \n//   http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\nval stopWordsFilter \u003d new StopWordsRemover()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"filteredWords\")\n  .setCaseSensitive(false)\n  \nval stopWords \u003d stopWordsFilter.getStopWords\nval newStopWords \u003d Array(\"did\", \"s\", \"t\", \"m\", \"n\", \"uh\", \"ll\", \"ha\", \"makes\", \"make\", \"yeah\", \"goes\", \n  \"gettin\", \"v\", \"went\", \"aint\", \"let\", \"d\", \"yer\", \"don\", \"got\", \"just\", \"ain\", \"ve\", \"come\", \"gonna\", \n  \"says\", \"oh\", \"like\", \"way\", \"little\", \"said\", \"cause\", \"know\", \"say\", \"long\", \"time\", \"day\", \"wanna\",\n  \"chorus\", \"repeat\", \"didn\", \"hey\", \"couldn\", \"wouldn\", \"ooh\", \"lets\", \"ol\", \"round\", \"right\", \"does\", \n  \"won\", \"ya\", \"outta\", \"tell\", \"doin\", \"doing\", \"boondocks\", \"nothin\", \"feelin\", \"mornin\", \"whoa\", \n  \"big\", \"havin\", \"comes\", \"sure\", \"yea\", \"mind\", \"best\", \"brr\", \"y\") ++ stopWords \n  \nstopWordsFilter.setStopWords(newStopWords)",
      "dateUpdated": "Mar 10, 2016 12:08:41 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872022_-2076523971",
      "id": "20160129-091432_1560645350",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.StopWordsRemover\nstopWordsFilter: org.apache.spark.ml.feature.StopWordsRemover \u003d stopWords_63a3af4523b2\nstopWords: Array[String] \u003d Array(a, about, above, across, after, afterwards, again, against, all, almost, alone, along, already, also, although, always, am, among, amongst, amoungst, amount, an, and, another, any, anyhow, anyone, anything, anyway, anywhere, are, around, as, at, back, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, below, beside, besides, between, beyond, bill, both, bottom, but, by, call, can, cannot, cant, co, con, could, couldnt, cry, de, describe, detail, do, done, down, due, during, each, eg, eight, either, eleven, else, elsewhere, empty, enough, etc, even, ever, every, everyone, everything, everywhere, except, few, fifteen, fify, fill, find, fire, first, five, for, former, formerly, forty, found, four, from, front, full, fur...newStopWords: Array[String] \u003d Array(did, s, t, m, n, uh, ll, ha, makes, make, yeah, goes, gettin, v, went, aint, let, d, yer, don, got, just, ain, ve, come, gonna, says, oh, like, way, little, said, cause, know, say, long, time, day, wanna, chorus, repeat, didn, hey, couldn, wouldn, ooh, lets, ol, round, right, does, won, ya, outta, tell, doin, doing, boondocks, nothin, feelin, mornin, whoa, big, havin, comes, sure, yea, mind, best, brr, y, a, about, above, across, after, afterwards, again, against, all, almost, alone, along, already, also, although, always, am, among, amongst, amoungst, amount, an, and, another, any, anyhow, anyone, anything, anyway, anywhere, are, around, as, at, back, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, below, besi...res43: stopWordsFilter.type \u003d stopWords_63a3af4523b2\n"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Mar 10, 2016 12:08:41 PM",
      "dateFinished": "Mar 10, 2016 12:08:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Word Count per document",
      "text": "import org.apache.spark.ml.feature.CountVectorizer\n\n// Limit to top `vocabSize` most common words and convert to word count vector features\nval vocabSize: Int \u003d 1000\n\nval countVectorizer \u003d new CountVectorizer()\n  .setInputCol(stopWordsFilter.getOutputCol)\n  .setOutputCol(\"countFeatures\")\n  .setVocabSize(vocabSize)\n  .setMinDF(3)\n  .setMinTF(2)",
      "dateUpdated": "Mar 10, 2016 12:08:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058997724_-1357596843",
      "id": "20160129-091637_724509031",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.feature.CountVectorizer\nvocabSize: Int \u003d 1000\ncountVectorizer: org.apache.spark.ml.feature.CountVectorizer \u003d cntVec_f101536cd2d3\n"
      },
      "dateCreated": "Jan 29, 2016 9:16:37 AM",
      "dateStarted": "Mar 10, 2016 12:08:47 PM",
      "dateFinished": "Mar 10, 2016 12:08:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create training pipeline",
      "text": "import org.apache.spark.ml.Pipeline\n\nval pipeline \u003d new Pipeline()\n  .setStages(Array(tokenizer, stopWordsFilter, countVectorizer))",
      "dateUpdated": "Mar 10, 2016 12:08:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872023_-2076908720",
      "id": "20160129-091432_1743901207",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.Pipeline\npipeline: org.apache.spark.ml.Pipeline \u003d pipeline_0d5303ab29e7\n"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Mar 10, 2016 12:08:58 PM",
      "dateFinished": "Mar 10, 2016 12:08:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "train the model",
      "text": "val model \u003d pipeline.fit(itemsDF)",
      "dateUpdated": "Mar 10, 2016 12:09:02 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872023_-2076908720",
      "id": "20160129-091432_1843245921",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "model: org.apache.spark.ml.PipelineModel \u003d pipeline_0d5303ab29e7\n"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Mar 10, 2016 12:09:02 PM",
      "dateFinished": "Mar 10, 2016 12:09:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Extract word distribution",
      "text": "import org.apache.spark.mllib.linalg.SparseVector\nimport org.apache.spark.ml.feature.CountVectorizerModel\n\nval countFeaturesDF \u003d model.transform(itemsDF).select($\"countFeatures\")\n\nval countVectorizerModel \u003d model.stages(2).asInstanceOf[CountVectorizerModel]\n\nval vocabulary \u003d countVectorizerModel.vocabulary\n\nval tuples \u003d countFeaturesDF.map(row \u003d\u003e {\n    val countVector \u003d row.getAs[SparseVector](0)\n    val indices \u003d countVector.indices\n    val values \u003d countVector.values\n    (indices.size, values.size)\n    val words \u003d indices.map(idx \u003d\u003e (vocabulary(idx)))\n    words.zip(values)\n})\n\nval wordCountDF \u003d tuples.map(_(0)).reduceByKey(_ + _).toDF(\"word\", \"count\").sort($\"count\" desc).limit(10)\nz.show(wordCountDF)",
      "dateUpdated": "Mar 10, 2016 12:09:47 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 388.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "word",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "word",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872023_-2076908720",
      "id": "20160129-091432_475951746",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "word\tcount\nlove\t122.0\ngone\t74.0\nbaby\t53.0\nlife\t33.0\nfeel\t23.0\neyes\t21.0\nheart\t11.0\ngod\t9.0\nold\t9.0\nwant\t6.0\n"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Mar 10, 2016 12:09:06 PM",
      "dateFinished": "Mar 10, 2016 12:09:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Feb 16, 2016 5:27:39 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1454058872028_-2080371460",
      "id": "20160129-091432_448644104",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jan 29, 2016 9:14:32 AM",
      "dateStarted": "Feb 16, 2016 5:28:06 PM",
      "dateFinished": "Feb 16, 2016 5:28:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "NLP/02: Unigram (Word) Distribution",
  "id": "2BAXQPPAV",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}