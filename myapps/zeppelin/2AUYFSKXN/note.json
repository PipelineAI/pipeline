{
  "paragraphs": [
    {
      "title": "Collaborative Filtering:  Matrix Factorization using Alternating Least Squares (ALS)",
      "text": "%md ![Alternating Least Squares - Matrix Factorization](http://advancedspark.com/img/collaborative-filtering-with-als-matrix-factorization.png)",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": true,
        "editorMode": "ace/mode/markdown",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435978153894_1534941045",
      "id": "20150704-024913_884517592",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"http://advancedspark.com/img/collaborative-filtering-with-als-matrix-factorization.png\" alt\u003d\"Alternating Least Squares - Matrix Factorization\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 4, 2015 2:49:13 AM",
      "dateStarted": "Apr 20, 2016 8:15:31 AM",
      "dateFinished": "Apr 20, 2016 8:15:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Reference Data for Enrichment",
      "text": "val itemsDF \u003d sqlContext.read.format(\"json\")\n  .load(\"file:/root/pipeline/html/advancedspark.com/json/software.json\")\n  .withColumnRenamed(\"id\", \"itemId\")\n  .as(\"items\")\n\nz.show(itemsDF.select($\"itemId\", $\"title\", $\"img\", $\"tags\"))",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 175.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itemId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "title",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456864386968_-1684206029",
      "id": "20160301-203306_1764877860",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "itemId\ttitle\timg\ttags\n1\tApache Cassandra\timg/software/cassandra.png\tWrappedArray(Database, NoSQL, Java, Eventually Consistent, Transactional)\n2\tTachyon\timg/software/tachyon.png\tWrappedArray(Distributed Cache, Object Store, S3, Swift, HDFS)\n3\tApache Ambari\timg/software/ambari.png\tWrappedArray(Cluster Provision, Hadoop, Cluster Monitoring, REST API, Metrics, Alerts)\n4\tDocker\timg/software/docker.png\tWrappedArray(Container, Linux, DevOps, Deployment)\n5\tMicrosft Azure\timg/software/azure.png\tWrappedArray(Cloud Provider, Microsoft)\n6\tApache Flink\timg/software/flink.png\tWrappedArray(Data Processing, Java, Scala, SQL, DataFrame, Table, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling)\n7\tApache Spark\timg/software/spark.png\tWrappedArray(Data Processing, Java, Scala, SQL, R, Python, DataFrame, Table, DataStream, Streaming Analytics, Batch Analytics, Machine Learning, Graph Analytics, Approximations, Sampling, Lazy)\n8\tApache Flume\timg/software/flume.png\tWrappedArray(Library, Java, Log Collection)\n9\tApache Giraph\timg/software/giraph.png\tWrappedArray(Library, Java, Graph Analytics, Batch)\n10\tApache HDFS\timg/software/hdfs.png\tWrappedArray(File System, Hadoop, Java)\n11\tApache YARN\timg/software/yarn.png\tWrappedArray(Cluster Resource Manager, Hadoop, Java)\n12\tApache HBase\timg/software/hbase.png\tWrappedArray(Database, Hadoop, NoSQL, Java, Eventually Consistent)\n13\tApache MapReduce\timg/software/mapreduce.png\tWrappedArray(Data Processing, Hadoop, Java, Python)\n14\tApache Hive\timg/software/hive.png\tWrappedArray(Data Processing, Hadoop, HiveQL, SQL, Query Processing, Java, MapReduce)\n15\tHortonworks\timg/software/hortonworks.png\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n16\tApache HUE\timg/software/hue.png\tWrappedArray(UI, Hadoop, Cloudera, Ad Hoc, HiveQL, SQL, Data Import, Java)\n17\tApache Impala\timg/software/impala.png\tWrappedArray(Data Processing, Query Processing, SQL, C++, Batch Analytics)\n18\tApache Kafka\timg/software/kafka.png\tWrappedArray(Message Broker, Java, C++, REST API, Messaging, Publish Subscribe, Producer Consumer)\n19\tApache Lucene\timg/software/lucene.png\tWrappedArray(Library, Search, Java, Python)\n20\tApache Solr\timg/software/solr.png\tWrappedArray(Search Engine, Java, REST API, UI, Python, Ruby, XML, JSON)\n21\tElasticSearch\timg/software/elasticsearch.png\tWrappedArray(Search Engine, Java, Python, REST API, Lucene, XML, JSON, Aggregations)\n22\tApache Mahout\timg/software/mahout.png\tWrappedArray(Library, Machine Learning, Java)\n23\tApache Drill\timg/software/drill.png\tWrappedArray(Data Processing, Query Processing, SQL, Aggregations, Joins, Batch Analytics)\n24\tApache Mesos\timg/software/mesos.png\tWrappedArray(Cluster Resource Manager, Docker, Container)\n25\tApache Parquet\timg/software/parquet.png\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema, Java, C++, Python)\n26\tApache ORC\timg/software/orc.png\tWrappedArray(File Format, Columnar, Compression, Evolving Schema, Nested Schema)\n27\tApache Pig\timg/software/pig.png\tWrappedArray(Data Processing, Hadoop, HiveQL, SQL, Query Processing, Java, Lazy)\n28\tApache ZooKeeper\timg/software/zookeeper.png\tWrappedArray(Distribured Coordinator, Paxos, RAFT, Hadoop, HiveQL, SQL, Query Processing, Java, Lazy)\n29\tStanford CoreNLP\timg/software/corenlp.png\tWrappedArray(Library, NLP, Java, Text Analytics)\n30\tApache Tez\timg/software/tez.png\tWrappedArray(Data Processing, Hadoop, YARN, Query Processing, Java, Lazy, HiveQL, Pig, SQL)\n31\tApache Storm\timg/software/storm.png\tWrappedArray(Streaming, Java)\n32\tApache Sqoop\timg/software/sqoop.png\tWrappedArray(Data Import, Hadoop, Java)\n33\tApache Oozie\timg/software/oozie.png\tWrappedArray(Workflow, Hadoop, Java, UI)\n34\tApache Nifi\timg/software/nifi.png\tWrappedArray(Workflow, Streaming, Message Broker, Java, UI)\n35\tNLTK\timg/software/nltk.png\tWrappedArray(Library, NLP, Python, Text Analytics)\n36\tSci-Kit Learn\timg/software/scikit-learn.png\tWrappedArray(Library, Python, Machine Learning)\n37\tiPython/Jupyter\timg/software/ipython.png\tWrappedArray(Notebook, Python, Java, Scala, R, Visualization, SQL)\n38\tApache Zeppelin\timg/software/zeppelin.png\tWrappedArray(Notebook, Python, Java, Scala, R, HiveQL, Cassandra, Visualization, SQL)\n39\tTableau\timg/software/tableau.png\tWrappedArray(BI, UI, Visualization, SQL)\n40\tR\timg/software/r.png\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n41\tSQL\timg/software/sql.png\tWrappedArray(Programming Language, SQL, RDBMS, Interpreted)\n42\tScala\timg/software/scala.png\tWrappedArray(Programming Language, Functional, Java, Static Typing, Compiled)\n43\tJava\timg/software/java.png\tWrappedArray(Programming Language, Object Oriented, Java, Static Typing, Compiled)\n44\tPython\timg/software/python.png\tWrappedArray(Programming Language, Dynamic Typing, Interpreted)\n45\tPresto\timg/software/presto.png\tWrappedArray(Data Processing, Query Processing, Java, SQL, Machine Learning)\n46\tMapR\timg/software/mapr.png\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n47\tCloudera\timg/software/cloudera.png\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n48\tIBM BigInsights\timg/software/biginsights.png\tWrappedArray(Distribution, Hadoop, Spark, Kafka)\n49\tAmazon Web Services\timg/software/aws.png\tWrappedArray(Cloud Provider, AWS)\n50\tGoogle Cloud Platform\timg/software/googlecloud.png\tWrappedArray(Cloud Provider, Google)\n51\tRedis\timg/software/redis.png\tWrappedArray(Distributed Cache, Key Value Store, HyperLogLog, Approximations, Probabilistic Data Structures, UDAF)\n52\tJSON\timg/software/json.png\tWrappedArray(File Format, Key Value Store)\n53\tXML\timg/software/xml.png\tWrappedArray(File Format, Key Value Store)\n54\tMongoDB\timg/software/mongodb.png\tWrappedArray(Database, Document Store, Key Value Store, NoSQL, JSON, Eventually Consistent)\n55\tOn-Premise\timg/software/onpremise.png\tWrappedArray(Cloud Provider, Data Center)\n56\tMicroStrategy\timg/software/microstrategy.png\tWrappedArray(BI, UI, Visualization, SQL)\n57\tKnime\timg/software/knime.png\tWrappedArray(Workflow, UI, Machine Learning, Graph Processing, Visualization)\n59\tOracle\timg/software/oracle.png\tWrappedArray(Database, SQL, RDBMS, Transactional)\n60\tMySQL\timg/software/mysql.png\tWrappedArray(Database, SQL, RDBMS, Transactional)\n61\tSpark ML/MLlib\timg/software/spark-ml.png\tWrappedArray(Library, Spark, Machine Learning)\n62\tSpark Streaming\timg/software/spark-streaming.png\tWrappedArray(Library, Spark, Streaming)\n63\tSpark SQL\timg/software/spark-sql.png\tWrappedArray(Library, Spark, HiveQL, SQL)\n64\tCSV\timg/software/csv.png\tWrappedArray(File Format)\n65\tDeep Learning 4J\timg/software/deeplearning4j.png\tWrappedArray(Library, Deep Learning, Neural Networks)\n66\tRedshift\timg/software/redshift.png\tWrappedArray(Database, Columnar, Data Warehouse, AWS, SQL)\n67\tKinesis\timg/software/kinesis.png\tWrappedArray(Library, Streaming, AWS)\n68\tDynamoDB\timg/software/dynamodb.png\tWrappedArray(Database, NoSQL, AWS, SQL, Approximations, Eventually Consistent)\n69\tSpark GraphX\timg/software/spark-graphx.png\tWrappedArray(Library, Graph Analytics, Spark)\n70\tSQL Server\timg/software/sqlserver.png\tWrappedArray(Database, SQL, Microsoft, RDBMS, Transactional)\n71\tElastic MapReduce\timg/software/emr.png\tWrappedArray(Data Processing, MapReduce, Spark, HiveQL, Pig, AWS, Presto)\n72\tDato GraphLab Create\timg/software/graphlab.png\tWrappedArray(Library, UI, Graph Analytics, Machine Learning, Query Processing, Visualization)\n73\tMemcached\timg/software/memcached.png\tWrappedArray(Distributed Cache, Key Value Store, Java, Python, C++)\n74\tNeo4j\timg/software/neo4j.png\tWrappedArray(Library, Graph Analytics, Java)\n75\tPostgres\timg/software/postgres.png\tWrappedArray(Database, SQL, RDBMS, Transactional)\n76\tProtobuffers\timg/software/protobuffers.png\tWrappedArray(File Format, Evolving Schema, Nested Schema)\n77\tS3\timg/software/s3.png\tWrappedArray(File System, Object Store, AWS, Eventually Consistent)\n78\tTensor Flow\timg/software/tensorflow.png\tWrappedArray(Data Processing, Deep Learning, Neural Networks)\n79\tTitan GraphDB\timg/software/titandb.png\tWrappedArray(Database, Graph, Graph Analytics, Java, Transactional)\n80\tTeradata\timg/software/teradata.png\tWrappedArray(Database, Data Warehouse, SQL)\n81\tVertica\timg/software/vertica.png\tWrappedArray(Database, Data Warehouse, SQL)\n"
      },
      "dateCreated": "Mar 1, 2016 8:33:06 PM",
      "dateStarted": "Apr 20, 2016 8:15:31 AM",
      "dateFinished": "Apr 20, 2016 8:15:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get Live Ratings from Cassandra",
      "text": "val cassandraConfig \u003d Map(\"keyspace\" -\u003e \"advancedspark\", \"table\" -\u003e \"item_ratings\")\n\nval itemRatingsDF \u003d sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\n  .options(cassandraConfig)\n  .load()\n  .toDF(\"userId\", \"itemId\", \"rating\", \"timestamp\")\n  .as(\"itemRatings\")",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456864425764_-1117794352",
      "id": "20160301-203345_1180596367",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "cassandraConfig: scala.collection.immutable.Map[String,String] \u003d Map(keyspace -\u003e advancedspark, table -\u003e item_ratings)\nitemRatingsDF: org.apache.spark.sql.DataFrame \u003d [userId: int, itemId: int, rating: int, timestamp: bigint]\n"
      },
      "dateCreated": "Mar 1, 2016 8:33:45 PM",
      "dateStarted": "Apr 20, 2016 8:15:31 AM",
      "dateFinished": "Apr 20, 2016 8:15:37 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train The ALS Model",
      "text": "import org.apache.spark.ml.recommendation.ALS\n\nval rank \u003d 10 // this is k\nval maxIterations \u003d 20\nval convergenceThreshold \u003d 0.01\n\nval als \u003d new ALS()\n  .setRank(rank)\n  .setRegParam(convergenceThreshold)\n  .setUserCol(\"userId\")\n  .setItemCol(\"itemId\")\n  .setRatingCol(\"rating\")\n\nval model \u003d als.fit(itemRatingsDF)",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1435978256373_-160526409",
      "id": "20150704-025056_169923529",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.ml.recommendation.ALS\nrank: Int \u003d 10\nmaxIterations: Int \u003d 20\nconvergenceThreshold: Double \u003d 0.01\nals: org.apache.spark.ml.recommendation.ALS \u003d als_8b97681da74f\nmodel: org.apache.spark.ml.recommendation.ALSModel \u003d als_8b97681da74f\n"
      },
      "dateCreated": "Jul 4, 2015 2:50:56 AM",
      "dateStarted": "Apr 20, 2016 8:15:36 AM",
      "dateFinished": "Apr 20, 2016 8:15:41 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Generate Recommendations for all Users",
      "text": "model.setPredictionCol(\"confidence\")\n\nval recommendationsDF \u003d model.transform(itemRatingsDF.select($\"userId\", $\"itemId\"))\n\nval enrichedRecommendationsDF \u003d \n   recommendationsDF.join(itemsDF, $\"itemId\" \u003d\u003d\u003d $\"id\")\n   .select($\"userId\", $\"itemId\", $\"title\", $\"description\", $\"tags\", $\"img\", $\"confidence\")\n   .sort($\"userId\", $\"itemId\", $\"confidence\" desc)\n   \nz.show(enrichedRecommendationsDF.select($\"userId\", $\"itemId\", $\"title\", $\"confidence\"))",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100304691_419185220",
      "id": "20160419-211144_1773438539",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:29: error: not found: value model\n              model.setPredictionCol(\"confidence\")\n              ^\n"
      },
      "dateCreated": "Apr 19, 2016 9:11:44 PM",
      "dateStarted": "Apr 20, 2016 8:10:32 AM",
      "dateFinished": "Apr 20, 2016 8:10:33 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Store full recommendations to disk",
      "text": "enrichedRecommendationsDF\n .select($\"userId\", $\"itemId\", $\"title\", $\"description\", $\"tags\", $\"img\", $\"confidence\")\n .sort($\"userId\", $\"itemId\", $\"confidence\" desc)\n .coalesce(1)\n .write.mode(\"overwrite\")\n .json(s\"\"\"/root/pipeline/work/serving/recommendations/als/full-recs/\"\"\")",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100402332_-1279142397",
      "id": "20160419-211322_1021941780",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:29: error: not found: value enrichedRecommendationsDF\nenrichedRecommendationsDF\n^\n"
      },
      "dateCreated": "Apr 19, 2016 9:13:22 PM",
      "dateStarted": "Apr 20, 2016 8:10:33 AM",
      "dateFinished": "Apr 20, 2016 8:10:33 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Store full recommendations in elasticSearch",
      "text": "import org.elasticsearch.spark.sql._ \nimport org.apache.spark.sql.SaveMode\n\nval esConfig \u003d Map(\"pushdown\" -\u003e \"true\", \"es.nodes\" -\u003e \"127.0.0.1\", \"es.port\" -\u003e \"9200\")\nenrichedRecommendationsDF.write.format(\"org.elasticsearch.spark.sql\").mode(SaveMode.Overwrite).options(esConfig)\n  .save(\"advancedspark/personalized-als\")",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461100478491_-968756780",
      "id": "20160419-211438_174483447",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.elasticsearch.spark.sql._\nimport org.apache.spark.sql.SaveMode\nesConfig: scala.collection.immutable.Map[String,String] \u003d Map(pushdown -\u003e true, es.nodes -\u003e 127.0.0.1, es.port -\u003e 9200)\n\u003cconsole\u003e:35: error: not found: value enrichedRecommendationsDF\n              enrichedRecommendationsDF.write.format(\"org.elasticsearch.spark.sql\").mode(SaveMode.Overwrite).options(esConfig)\n              ^\n"
      },
      "dateCreated": "Apr 19, 2016 9:14:38 PM",
      "dateStarted": "Apr 20, 2016 8:10:34 AM",
      "dateFinished": "Apr 20, 2016 8:10:34 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show User Factors Matrix",
      "text": "val userFactorsDF \u003d model.userFactors.map(row \u003d\u003e \n  (row.getInt(0),  Vectors.dense(row.getSeq[Float](1).toArray.map(_.toDouble)))\n).toDF(\"userId\", \"userFactors\")\n .sort($\"userId\" asc)\n .as(\"userFactors\")\n\nz.show(userFactorsDF)",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "userFactors",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "userId",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "userFactors",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456873568687_1047705598",
      "id": "20160301-230608_2073461478",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value model\n         val userFactorsDF \u003d model.userFactors.map(row \u003d\u003e \n                             ^\n"
      },
      "dateCreated": "Mar 1, 2016 11:06:08 PM",
      "dateStarted": "Apr 20, 2016 8:10:34 AM",
      "dateFinished": "Apr 20, 2016 8:10:34 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show ItemFactors Matrix",
      "text": "val itemFactorsDF \u003d model.itemFactors.map(row \u003d\u003e\n  (row.getInt(0),  Vectors.dense(row.getSeq[Float](1).toArray.map(_.toDouble)))\n).toDF(\"itemId\", \"itemFactors\")\n .sort($\"itemId\" asc)\n .as(\"itemFactors\")\n\nz.show(itemFactorsDF)",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 314.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456865217092_774550692",
      "id": "20160301-204657_541485869",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value model\n         val itemFactorsDF \u003d model.itemFactors.map(row \u003d\u003e\n                             ^\n"
      },
      "dateCreated": "Mar 1, 2016 8:46:57 PM",
      "dateStarted": "Apr 20, 2016 8:10:35 AM",
      "dateFinished": "Apr 20, 2016 8:10:35 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Enriched the Item Factors",
      "text": "val enrichedItemFactorsDF \u003d itemFactorsDF\n  .join(itemsDF, $\"items.itemId\" \u003d\u003d\u003d $\"itemFactors.itemId\")\n  .select($\"items.itemId\", $\"title\", $\"tags\", $\"itemFactors\")\n  .sort($\"items.itemId\")\n  .as(\"enrichedItemFactors\")\n\nz.show(enrichedItemFactorsDF)",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1461098265813_1981370856",
      "id": "20160419-203745_842155819",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:32: error: not found: value itemFactorsDF\n         val enrichedItemFactorsDF \u003d itemFactorsDF\n                                     ^\n"
      },
      "dateCreated": "Apr 19, 2016 8:37:45 PM",
      "dateStarted": "Apr 20, 2016 8:10:35 AM",
      "dateFinished": "Apr 20, 2016 8:10:35 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Write the User and item factor vectors to elasticSearch",
      "text": "val esConfig \u003d Map(\"pushdown\" -\u003e \"true\", \"es.nodes\" -\u003e \"127.0.0.1\", \"es.port\" -\u003e \"9200\")\n\nuserFactorsDF.select($\"userId\", $\"userFactors\")\n .sort($\"userId\" asc)\n .write.format(\"org.elasticsearch.spark.sql\")\n .mode(\"overwrite\")\n .options(esConfig)\n .save(\"advancedspark/user-factors-als\")\n \nenrichedItemFactorsDF.select($\"itemId\", $\"title\", $\"itemFactors\", $\"description\", $\"tags\", $\"img\")\n .sort($\"itemId\" asc)\n .write.format(\"org.elasticsearch.spark.sql\")\n .mode(\"overwrite\")\n .options(esConfig)\n .save(\"advancedspark/item-factors-als\")",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460932159717_2026735022",
      "id": "20160417-222919_647801278",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "esConfig: scala.collection.immutable.Map[String,String] \u003d Map(pushdown -\u003e true, es.nodes -\u003e 127.0.0.1, es.port -\u003e 9200)\n\u003cconsole\u003e:36: error: not found: value userFactorsDF\n              userFactorsDF.select($\"userId\", $\"userFactors\")\n              ^\n"
      },
      "dateCreated": "Apr 17, 2016 10:29:19 PM",
      "dateStarted": "Apr 20, 2016 8:10:37 AM",
      "dateFinished": "Apr 20, 2016 8:10:37 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate Confidence Given userId and ItemId",
      "text": "////////////////////////////////////////////\n// TODO:  Must Update Item ID and User ID //\n////////////////////////////////////////////\n\nimport org.jblas.DoubleMatrix\n\n// Create JBlas DoubleMatrix from features\n\n// User 12663\nval givenUserId \u003d 12663\nval givenUserFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n  .where($\"userId\" \u003d\u003d\u003d givenUserId)\n  .collect()(0)\n\n// Item 7\nval givenItemId \u003d 7\nval givenItemFactors \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n  .where($\"itemId\" \u003d\u003d\u003d givenItemId)\n  .collect()(0)\n  \nval userFactors \u003d new DoubleMatrix(givenUserFactors)\nval itemFactors \u003d new DoubleMatrix(givenItemFactors)\n\n// Take dot product of the User x Item vectors\n// This should equal the confidence value in the offline-generated matrix\nval confidence \u003d userFactors.dot(itemFactors)\n\n////////////////////////////////////////////\n// TODO:  Must Update Item ID and User ID //\n////////////////////////////////////////////",
      "dateUpdated": "Apr 20, 2016 8:15:31 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456954812726_1681075178",
      "id": "20160302-214012_540640872",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.jblas.DoubleMatrix\ngivenUserId: Int \u003d 12663\n\u003cconsole\u003e:35: error: not found: value model\n         val givenUserFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n                                ^\n"
      },
      "dateCreated": "Mar 2, 2016 9:40:12 PM",
      "dateStarted": "Apr 20, 2016 8:10:37 AM",
      "dateFinished": "Apr 20, 2016 8:10:38 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Top 5 Similar Items to a given item",
      "text": "import org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity \n\n// Given Item: 7 (Spark)\nval givenItemId \u003d 7\n\nval givenItemLatentFactors \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n  .where($\"itemId\" \u003d\u003d\u003d givenItemId)\n  .map(row \u003d\u003e (row.getSeq[Float](1).toArray.map(_.toDouble)))\n  .collect()\n\n// Convert Array[Double] to DoubleMatrix\nval givenItemLatentFactorVector \u003d new DoubleMatrix(givenItemLatentFactors)\n\n// Find Similar Items to the Given Item\nval similarItems \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n  .filter($\"itemId\" !\u003d\u003d givenItemId)\n  .map{ row \u003d\u003e\n     val itemId \u003d row.getInt(0)\n     val itemLatentFactor \u003d row.getSeq[Float](1).map(_.toDouble).toArray\n     val itemLatentFactorVector \u003d new DoubleMatrix(itemLatentFactor)\n     val similarity \u003d Similarity.cosineSimilarity(itemLatentFactorVector, givenItemLatentFactorVector)\n     (itemId, similarity)\n  }\n  \n// Sort and Return Top 5 Items by Similarity to Given Item\nval sortedSimilarItems \u003d similarItems.top(5)(Ordering.by[(Int, Double), Double] { case (id, similarity) \u003d\u003e similarity })\n\nval sortedSimilarItemsDF \u003d sqlContext.createDataFrame(sortedSimilarItems).toDF(\"similarItemId\", \"similarity\")\n\nval enrichedSortedSimilarItemsDF \u003d \n   sortedSimilarItemsDF.join(itemsDF, $\"similarItemId\" \u003d\u003d\u003d $\"id\")\n   .select($\"similarItemId\", $\"title\", $\"description\", $\"tags\", $\"img\", $\"similarity\")\n   .sort($\"similarity\" desc)\n   \nz.show(enrichedSortedSimilarItemsDF.select(lit(givenItemId).as(\"itemId\"), $\"similarItemId\", $\"title\", $\"similarity\"))",
      "dateUpdated": "Apr 20, 2016 8:15:32 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 182.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460866272937_387648933",
      "id": "20160417-041112_1866839334",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity\ngivenItemId: Int \u003d 7\n\u003cconsole\u003e:38: error: not found: value model\n       val givenItemLatentFactors \u003d model.itemFactors.select($\"id\".as(\"itemId\"), $\"features\".as(\"itemFeatures\"))\n                                    ^\n"
      },
      "dateCreated": "Apr 17, 2016 4:11:12 AM",
      "dateStarted": "Apr 20, 2016 8:10:38 AM",
      "dateFinished": "Apr 20, 2016 8:10:38 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Top 5 Similar Users to a given user",
      "text": "/////////////////////////////////////\n// May Need to Update User ID      //\n/////////////////////////////////////\n\n// User 12663\nimport org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity \n\n// Given User: 12663\nval givenUserId \u003d 12663\nval givenUserLatentFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n  .where($\"userId\" \u003d\u003d\u003d givenUserId)\n  .map(row \u003d\u003e (row.getSeq[Float](1).toArray.map(_.toDouble)))\n  .collect()(0)\n\n// Convert Array[Double] to DoubleMatrix\nval givenUserLatentFactorVector \u003d new DoubleMatrix(givenUserLatentFactors)\n\n// Find Similar Users to the Given User\nval similarUsers \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n  .filter($\"userId\" !\u003d\u003d givenUserId)\n  .map{ row \u003d\u003e\n     val userId \u003d row.getInt(0)\n     val userLatentFactor \u003d row.getSeq[Float](1).map(_.toDouble).toArray\n     val userLatentFactorVector \u003d new DoubleMatrix(userLatentFactor)\n     val similarity \u003d Similarity.cosineSimilarity(userLatentFactorVector, givenUserLatentFactorVector)\n     (userId, similarity)\n  }\n  \n// Sort and Return Top 5 Users by Similarity to Given Users\nval sortedSimilarUsers \u003d similarUsers.top(5)(Ordering.by[(Int, Double), Double] { \n  case (id, similarity) \u003d\u003e similarity \n}).mkString(\"\\n\")\n\n/////////////////////////////////\n// Must Update User ID         //\n/////////////////////////////////",
      "dateUpdated": "Apr 20, 2016 8:15:32 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460868286479_-213464598",
      "id": "20160417-044446_1052340818",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.Row\nimport com.advancedspark.ml.Similarity\ngivenUserId: Int \u003d 12663\n\u003cconsole\u003e:40: error: not found: value model\n         val givenUserLatentFactors \u003d model.userFactors.select($\"id\".as(\"userId\"), $\"features\".as(\"userFeatures\"))\n                                      ^\n"
      },
      "dateCreated": "Apr 17, 2016 4:44:46 AM",
      "dateStarted": "Apr 20, 2016 8:10:38 AM",
      "dateFinished": "Apr 20, 2016 8:10:39 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Apr 20, 2016 8:15:32 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1460922825977_-689845777",
      "id": "20160417-195345_2022099383",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Apr 17, 2016 7:53:45 PM",
      "dateStarted": "Apr 20, 2016 8:10:39 AM",
      "dateFinished": "Apr 20, 2016 8:10:39 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Live Recs/03:  Generate User-to-Item Collaborative Filter Recs (ALS)",
  "id": "2AUYFSKXN",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}