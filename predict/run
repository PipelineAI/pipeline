#!/bin/bash

start_guild_ui() {
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo ""
    echo "Starting UI..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME

#    cd $PIPELINE_MODEL_PATH
#    guild view &
#    cd /root
}

start_model_serving_python () {
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_SERVER_PATH=$PIPELINE_MODEL_SERVER_PATH"
    echo "PIPELINE_MODEL_SERVER_PORT=$PIPELINE_MODEL_SERVER_PORT"
    echo "PIPELINE_MODEL_SERVER_PROMETHEUS_PORT=$PIPELINE_MODEL_SERVER_PROMETHEUS_PORT"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT"

    echo ""
    echo "Starting Python-based Model Serving..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME

    # TODO:  is this cd needed?
    cd $PIPELINE_MODEL_PATH \
      && PYTHONPATH=$PIPELINE_MODEL_PATH:$PYTHONPATH \
        $PIPELINE_MODEL_SERVER_PATH/model_server_python.py \
        --PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE \
        --PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME \
        --PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH \
        --PIPELINE_MODEL_SERVER_PORT=$PIPELINE_MODEL_SERVER_PORT \
        --PIPELINE_MODEL_SERVER_PROMETHEUS_PORT=$PIPELINE_MODEL_SERVER_PROMETHEUS_PORT \
        --PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT

    cd /root
}

start_drop_server () {
    echo "PIPELINE_DROP_PATH=$PIPELINE_DROP_PATH"
    echo "PIPELINE_DROP_SERVER_PATH=$PIPELINE_DROP_SERVER_PATH"
    echo "PIPELINE_DROP_SERVER_PORT=$PIPELINE_DROP_SERVER_PORT"

    echo ""
    echo "Starting Python-based Drop Server..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME

    $PIPELINE_DROP_SERVER_PATH/drop_server_python.py \
      --PIPELINE_DROP_PATH=$PIPELINE_DROP_PATH \
      --PIPELINE_DROP_SERVER_PORT=$PIPELINE_DROP_SERVER_PORT
}

start_model_serving_jvm () {
    # TODO:  Log Relevant Stuff
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_SERVER_PORT=$PIPELINE_MODEL_SERVER_PORT"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT"

    export JAVA_MAX_MEM_RATIO=85
    export JAVA_OPTIONS="$(sysutils/jvm-limits.sh)"
    echo ""
    echo "JAVA_OPTIONS=$JAVA_OPTIONS"
    echo ""
    echo "Starting JVM-based Model Serving..."
    echo ""
    cd $PIPELINE_JVM_MODEL_SERVER_PATH \
      && java $JAVA_OPTIONS -Djava.security.egd=file:/dev/./urandom \
        -jar /root/jvm/lib/sbt-launch.jar \
        "runMain io.pipeline.predict.jvm.PredictionServiceMain"
}

start_tensorflow_serving () {
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING"
    echo ""
    echo "Starting TensorFlow Serving..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME
    tensorflow_model_server --port=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT \
        --model_name=$PIPELINE_MODEL_NAME --model_base_path=$PIPELINE_MODEL_PATH/versions \
        --batching_parameters_file=/root/config/tfserving/batching_config.txt \
        --enable_batching=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING \
        --file_system_poll_wait_seconds=5
    echo ""
    echo "...TensorFlow Serving Started!"
    echo ""
}

start_tensorboard() {
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_TENSORBOARD_LOGGER=$PIPELINE_TENSORBOARD_LOGGER"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING"
    echo ""
    echo "Starting TensorBoard..."
    echo ""
    source activate $PIPELINE_CONDA_ENV_NAME
    tensorboard --reload_interval=30 --logdir=$TENSORBOARD_LOGDIR
    echo ""
    echo "...TensorBoard Started!"
    echo ""
}

start_websocket_kafka_log_stream() {
    # TODO:  Log relevant stuff
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_WEBSOCKET_KAFKA_SERVER_PATH=$PIPELINE_WEBSOCKET_KAFKA_SERVER_PATH"
    echo "PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT=$PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT"
    echo "PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL=$PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL"
    echo ""
    echo "Starting WebSocket Kafka Log Stream Server..."
    echo ""
    source activate $PIPELINE_CONDA_ENV_NAME
    # Start Python, WebSocket, Tornado--based Kafka Log Stream Server
    python $PIPELINE_WEBSOCKET_KAFKA_SERVER_PATH/ws_kafka_topic_stream.py --PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT=$PIPELINE_WEBSOCKET_KAFKA_SERVER_PORT --PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL=$PIPELINE_WEBSOCKET_KAFKA_SERVER_UPDATE_INTERVAL
    echo ""
    echo "...Server Started!"
    echo ""
}

source sysutils/container-limits.sh

# Start Nginx Server
service nginx start

# TODO:  This is not needed for drop server
# Start Prometheus Metrics Server
prometheus -config.file=/root/prometheus-1.7.1.linux-amd64/prometheus.yml &

# TODO:  This is not needed for drop server
# Start Grafana Dashboard Server
cd /root/grafana-$GRAFANA_VERSION/ && bin/grafana-server web &

# TODO:  This is not needed for drop server
# Start Kafka REST API (includes Kafka Broker and Kafka Schema Registry)
cd ~
confluent start kafka &

echo "Required Environment Variables..."
echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"

echo ""
echo "___________________________________________"
echo " __     __   ___              ___          "
echo "|__) | |__) |__  |    | |\ | |__      /\  |"
echo "|    | |    |___ |___ | | \| |___    /~~\ |"
echo "___________________________________________"
echo ""

#if [[ $PIPELINE_MODEL_TYPE == "python" ]] ||
#   [[ $PIPELINE_MODEL_TYPE == "keras" ]] ||
#   [[ $PIPELINE_MODEL_TYPE == "scikit" ]]; then

  start_websocket_kafka_log_stream &

  start_model_serving_python &
#fi;

#if [[ $PIPELINE_MODEL_TYPE == "drop" ]]; then

  start_drop_server &
#fi;

#if [[ $PIPELINE_MODEL_TYPE == "tensorflow" ]]; then
  [ -s $PIPELINE_MODEL_PATH/pipeline_train.py ] \
    && start_tensorboard &

#  [ -s $PIPELINE_MODEL_PATH/Guild ] \
#    && start_guild_ui

  [ -s $PIPELINE_MODEL_PATH/versions ] \
    && start_tensorflow_serving &

#  start_model_serving_python &

#  start_websocket_kafka_log_stream &
#fi;

#if [[ $PIPELINE_MODEL_TYPE == "spark" ]] ||
#   [[ $PIPELINE_MODEL_TYPE == "pmml" ]] ||
#   [[ $PIPELINE_MODEL_TYPE == "java" ]] ||
#   [[ $PIPELINE_MODEL_TYPE == "xgboost" ]] ||
#   [[ $PIPELINE_MODEL_TYPE == "r" ]]; then

  start_model_serving_jvm
#fi;
