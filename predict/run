#!/bin/bash

start_model_training_tensorflow() {
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo ""
    echo "Starting TensorFlow-based Model Training..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME

    cd $PIPELINE_MODEL_PATH 
    guild prepare
    guild train --flag=learning_rate=0.05
    guild train --flag=learning_rate=0.025
    guild view &
    cd /root
}

start_model_training_python3() {
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"

    echo ""
    echo "Starting Python-based Model Training..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME

    cd $PIPELINE_MODEL_PATH && python pipeline_train.py && cd /root
    echo ""
    echo "...Training Complete!"
    echo ""
}

start_model_serving_python3 () {
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_SERVER_PATH=$PIPELINE_MODEL_SERVER_PATH"
    echo "PIPELINE_MODEL_SERVER_PORT=$PIPELINE_MODEL_SERVER_PORT"
    echo "PIPELINE_MODEL_SERVER_PROMETHEUS_PORT=$PIPELINE_MODEL_SERVER_PROMETHEUS_PORT"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT"

    echo ""
    echo "Starting Python-based Model Serving..."
    echo ""

    source activate $PIPELINE_CONDA_ENV_NAME 

    cd $PIPELINE_MODEL_PATH \
      && PYTHONPATH=$PIPELINE_MODEL_PATH:$PYTHONPATH \
        $PIPELINE_MODEL_SERVER_PATH/model_server_python3.py \
        --PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE \
        --PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME \
        --PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH \
        --PIPELINE_MODEL_SERVER_PORT=$PIPELINE_MODEL_SERVER_PORT \
        --PIPELINE_MODEL_SERVER_PROMETHEUS_PORT=$PIPELINE_MODEL_SERVER_PROMETHEUS_PORT \
        --PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT

    cd /root
}

start_model_serving_jvm () {
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_SERVER_PORT=$PIPELINE_MODEL_SERVER_PORT"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT"

    export JAVA_MAX_MEM_RATIO=85
    export JAVA_OPTIONS="$(sysutils/jvm-limits.sh)"

    echo ""
    echo "Starting JVM-based Model Serving..."
    echo ""
    cd $PIPELINE_MODEL_PATH \
      && java $JAVA_OPTIONS -Djava.security.egd=file:/dev/./urandom \
        -jar /root/jvm/lib/sbt-launch.jar \
        "run-main io.pipeline.prediction.jvm.PredictionServiceMain"
}

start_tensorflow_serving_in_background () {
    echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
    echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
    echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
    echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT"
    echo "PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING"
    echo ""
    echo "Starting TensorFlow Serving..."
    echo ""
    tensorflow_model_server --port=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_PORT \
        --model_name=$PIPELINE_MODEL_NAME --model_base_path=$PIPELINE_MODEL_PATH/versions \
        --batching_parameters_file=/root/config/tf_serving/batching_config.txt \
        --enable_batching=$PIPELINE_MODEL_SERVER_TENSORFLOW_SERVING_REQUEST_BATCHING \
        --file_system_poll_wait_seconds=5 &
    echo ""
    echo "...TensorFlow Serving Started!"
    echo ""
}

source sysutils/container-limits.sh

sudo service nginx start

prometheus -config.file=/root/prometheus-1.7.1.linux-amd64/prometheus.yml &

sudo service grafana-server start

#[ -s /root/nvidia/nvidia-prometheus-stats.py ] \
#  && python2 /root/nvidia/nvidia-prometheus-stats.py -u 1.0 -p 7070 &

echo "Required Environment Variables..."
echo "PIPELINE_MODEL_TYPE=$PIPELINE_MODEL_TYPE"
echo "PIPELINE_MODEL_NAME=$PIPELINE_MODEL_NAME"
echo "PIPELINE_MODEL_CHIP=$PIPELINE_MODEL_CHIP"
echo "PIPELINE_MODEL_PATH=$PIPELINE_MODEL_PATH"

if [[ $PIPELINE_MODEL_TYPE == "python3" ]] ||
   [[ $PIPELINE_MODEL_TYPE == "keras" ]] ||
   [[ $PIPELINE_MODEL_TYPE == "scikit" ]]; then

  [ -s $PIPELINE_MODEL_PATH/pipeline_train.py ] \
    && start_model_training_python3

  start_model_serving_python3
fi;

if [[ $PIPELINE_MODEL_TYPE == "tensorflow" ]]; then
  [ -s $PIPELINE_MODEL_PATH/pipeline_train.py ] \
    && start_model_training_tensorflow

  start_tensorflow_serving_in_background 
  start_model_serving_python3
fi;

if [[ $PIPELINE_MODEL_TYPE == "spark" ]] ||
   [[ $PIPELINE_MODEL_TYPE == "pmml" ]] ||
   [[ $PIPELINE_MODEL_TYPE == "java" ]] ||
   [[ $PIPELINE_MODEL_TYPE == "xgboost" ]] ||
   [[ $PIPELINE_MODEL_TYPE == "r" ]]; then

  start_model_serving_jvm
fi;
