include "application"

manager {
  notebooks {
    dir="/root/pipeline/notebooks/spark-notebook" # we can change that to an empty one (or copy the existing ones to it too)

    custom {
      localRepo="/root/.ivy2"
      sparkConf= {
        "spark.cassandra.connection.host": "127.0.0.1",
        "spark.master": "spark://127.0.0.1:7077",
        "spark.executor.cores": "2",
        "spark.executor.memory": "512m",
        "spark.cores.max": "2",
        "spark.eventLog.enabled": "true",
        "spark.eventLog.dir": "logs/spark"
      }
    }
  }

  clusters = {
    profiles: "/root/spark-notebook-0.6.0-scala-2.10.4-spark-1.4.1-hadoop-2.6.0-with-hive-with-parquet/conf/profiles"
    file: "/root/spark-notebook-0.6.0-scala-2.10.4-spark-1.4.1-hadoop-2.6.0-with-hive-with-parquet/conf/clusters"
  }

  tachyon.enabled=false
}
