{
  "paragraphs": [
    {
      "text": "// Databricks notebook source exported at Tue, 27 Oct 2015 09:38:07 UTC\n// MAGIC %md\n// MAGIC # ETL and K-Means\n// MAGIC  \n// MAGIC This lab will demonstrate loading data from a file, transforming that data into a form usable with the ML and MLlib libraries, and building a k-means clustering using both ML and MLlib.\n// MAGIC  \n// MAGIC Upon completing this lab you should understand how to read from and write to files in Spark, convert between `RDDs` and `DataFrames`, and build a model using both the ML and MLlib APIs.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Loading the data\n// MAGIC  \n// MAGIC First, we need to load data into Spark.  We\u0027ll use a built-in utility to load a [libSVM file](www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html), which is stored in an S3 bucket on AWS.  We\u0027ll use `MLUtils.loadLibSVMFile` to load our file.  Here are the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.util.MLUtils.loadLibSVMFile) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.MLUtils$) APIs.\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.util.MLUtils\n \nval baseDir \u003d \"/root/pipeline/datasets/misc/\"\nval irisPath \u003d baseDir + \"iris.scale\"\nval irisRDD \u003d MLUtils.loadLibSVMFile(sc, irisPath, 4, 20).cache\n \n// We get back an RDD of LabeledPoints.  Note that the libSVM format uses SparseVectors.\nirisRDD.take(5)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC What if we wanted to see the first few lines of the libSVM file to see what the format looks like?\n\n// COMMAND ----------\n\nsc.textFile(irisPath).take(5).mkString(\"\\n\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC How is this data stored across partitions?\n\n// COMMAND ----------\n\nprintln(s\"number of partitions: ${irisRDD.partitions.size}\")\nval elementsPerPart \u003d irisRDD\n  .mapPartitionsWithIndex( (i, x) \u003d\u003e Iterator((i, x.toArray.size)))\n  .collect()\nirisRDD.glom().first.mkString(\"\\n\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s convert this `RDD` of `LabeledPoints` to a `DataFrame`\n\n// COMMAND ----------\n\nval irisDF \u003d irisRDD.toDF()\nirisDF.take(5).mkString(\"\\n\")\n\n// COMMAND ----------\n\nirisDF.show(truncate\u003dfalse)\n\n// COMMAND ----------\n\n//display(irisDF)\n\n// COMMAND ----------\n\nprintln(irisDF.schema + \"\\n\")\nirisDF.printSchema\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Why were we able to convert directly from a `LabeledPoint` to a `Row`?\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC RDD containing Python object to a DataFrame\n// MAGIC  \n// MAGIC [createDataFrame](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/context.py#L342)\n// MAGIC  \n// MAGIC [_createFromRDD](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/context.py#L280)\n// MAGIC  \n// MAGIC [_inferSchema](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/context.py#L221)\n// MAGIC  \n// MAGIC [_infer_schema](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/types.py#L813)\n// MAGIC  \n// MAGIC [back to _createFromRDD](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/context.py#L304)\n// MAGIC  \n// MAGIC [toInternal](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/types.py#L533)\n// MAGIC  \n// MAGIC [back to createDataFrame](https://github.com/apache/spark/blob/3a11e50e21ececbec9708eb487b08196f195cd87/python/pyspark/sql/context.py#L404)\n\n// COMMAND ----------\n\ncase class Person(name: String, age: Int)\n\n// COMMAND ----------\n\nval personDF \u003d sqlContext.createDataFrame(Seq(Person(\"Bob\", 28), Person(\"Julie\", 25)))\npesronDF.show()\n//display(personDF)\n\n// COMMAND ----------\n\n// Show the schema that was inferred\nprintln(personDF.schema)\npersonDF.printSchema\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Transform the data\n// MAGIC  \n// MAGIC If you look at the data you\u0027ll notice that there are three values for label: 1, 2, and 3.  Spark\u0027s machine learning algorithms expect a 0 indexed target variable, so we\u0027ll want to adjust those labels.  This transformation is a simple expression where we\u0027ll subtract one from our `label` column.\n// MAGIC  \n// MAGIC For help reference the SQL Programming Guide portion on [dataframe-operations](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-operations) or the Spark SQL [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package) APIs.  `select`, `col`, and `alias` can be used to accomplish this.\n// MAGIC  \n// MAGIC The resulting `DataFrame` should have two columns: one named `features` and another named `label`.\n\n// COMMAND ----------\n\n// ANSWER\nimport org.apache.spark.sql.functions.col\n \nval irisDFZeroIndex \u003d irisDF.select($\"features\", (col(\"label\") - 1).alias(\"label\"))\nirisDFZeroIndex.show()\n\n// COMMAND ----------\n\nassert(irisDFZeroIndex.select(\"label\").map(_(0)).take(3).deep \u003d\u003d Array(0, 0, 0).deep,\n       \"incorrect value for irisDFZeroIndex\")\n\n// COMMAND ----------\n\n// You can use $ as a shortcut for a column.  Using $ creates a ColumnName which extends Column, so it can be used where Columns are expected\n$\"features\"\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC You\u0027ll also notice that we have four values for features and that those values are stored as a `SparseVector`.  We\u0027ll reduce those down to two values (for visualization purposes) and convert them to a `DenseVector`.  To do that we\u0027ll need to create a `udf` and apply it to our dataset.  Here\u0027s a `udf` reference for [Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf) and for [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.UserDefinedFunction).\n// MAGIC  \n// MAGIC Note that you can call the `toArray` method on a `SparseVector` to obtain an array, and you can convert an array into a `DenseVector` using the `Vectors.dense` method.\n\n// COMMAND ----------\n\n// ANSWER\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\n \n// Take the first two values from a SparseVector and convert them to a DenseVector\nval firstTwoFeatures \u003d udf { (vector: Vector) \u003d\u003e Vectors.dense(Array(vector(0), vector(1))) }\n \nval irisTwoFeaturesUDF \u003d irisDFZeroIndex.select(firstTwoFeatures($\"features\").as(\"features\"),  $\"label\").cache()\nirisTwoFeaturesUDF.show()\n\n// COMMAND ----------\n\n// TEST\nassert(irisTwoFeaturesUDF.first.toString() \u003d\u003d \"[[-0.555556,0.25],0.0]\",\n       \"incorrect definition of firstTwoFeatures\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC ## Part 2\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.feature.VectorSlicer\nval vs \u003d new VectorSlicer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"twoFeatures\")\n  .setIndices(Array(0, 1))\nval irisTwoFeaturesSlicer \u003d vs.transform(irisDFZeroIndex)\n  .select($\"twoFeatures\".as(\"features\"), $\"label\")\n\n// COMMAND ----------\n\n// VectorSlicer returns a SparseVector\nirisTwoFeaturesSlicer.first().getAs[Vector](0)\n\n// COMMAND ----------\n\n// Let\u0027s create a UDF to generate a DenseVector instead\nval toDense \u003d udf { (vector: Vector) \u003d\u003e vector.toDense }\n \nval irisTwoFeatures \u003d irisTwoFeaturesSlicer.select(toDense($\"features\").as(\"features\"), $\"label\")\nirisTwoFeatures.first.getAs[Vector](0)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s view our `irisTwoFeatures` `DataFrame`.\n\n// COMMAND ----------\n\nirisTwoFeatures.take(5).mkString(\"\\n\")\n\n// COMMAND ----------\n\nirisTwoFeatures.show()\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Saving our DataFrame\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We\u0027ll be using parquet files to save our data.  More information about the parquet file format can be found on [parquet.apache.org](https://parquet.apache.org/documentation/latest/).\n\n// COMMAND ----------\n\nimport scala.util.Random\nval id \u003d Random.nextLong.toString\nirisTwoFeatures.write.mode(\"overwrite\").parquet(s\"/tmp/$id/irisTwoFeatures.parquet\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Note that we\u0027ll get a part file for each partition and that these files are compressed.\n\n// COMMAND ----------\n\n//display(dbutils.fs.ls(baseDir + \"irisTwoFeatures.parquet\"))\n//display(dbutils.fs.ls(s\"/tmp/$id/irisTwoFeatures.parquet\"))\n\n// COMMAND ----------\n\nirisDFZeroIndex.write.mode(\"overwrite\").parquet(s\"/tmp/$id/irisFourFeatures.parquet\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### K-Means\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now we\u0027ll build a k-means model using our two features and inspect the class hierarchy.\n// MAGIC  \n// MAGIC We\u0027ll build the k-means model using `KMeans`, an `ml` `Estimator`.  Details can be found in the [Python](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.clustering) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.package) APIs.  Also, examples that use [PCA](http://spark.apache.org/docs/latest/ml-features.html#pca) and  [logistic regression](http://spark.apache.org/docs/latest/ml-guide.html#example-estimator-transformer-and-param) can be found in the ML Programming Guide.\n// MAGIC  \n// MAGIC Make sure to work with the `irisTwoFeatures` `DataFrame`.\n\n// COMMAND ----------\n\nimport org.apache.spark.ml.clustering.KMeans\n \n// Create a KMeans Estimator and set k\u003d3, seed\u003d5, maxIter\u003d20, initSteps\u003d1\nval kmeans \u003d new KMeans()\n  .setK(3)\n  .setSeed(5)\n  .setMaxIter(20)\n  .setInitSteps(1)\n \n// Call fit on the estimator and pass in our DataFrame\nval model \u003d kmeans.fit(irisTwoFeatures)\n \n// Obtain the clusterCenters from the KMeansModel\nval centers \u003d model.clusterCenters\n \n// Use the model to transform the DataFrame by adding cluster predictions\nval transformed \u003d model.transform(irisTwoFeatures)\n\n// COMMAND ----------\n\n// TEST\nimport org.apache.spark.mllib.linalg.Vectors\n \nassert(math.round(centers(0)(0)* 1000) \u003d\u003d 351,\n       \"incorrect centers.  check your params.\")\nassert(transformed.select(\"prediction\").map(_(0)).take(4).deep \u003d\u003d Array(1,1,1,1).deep,\n       \"incorrect predictions\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC ## PART 3\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC From the class hierarchy it is clear that `KMeans` is an `Estimator` while `KMeansModel` is a `Transformer`.\n\n// COMMAND ----------\n\nimport scala.reflect.runtime.universe._\n \ndef typesOf[T : TypeTag](v: T): List[Type] \u003d\n  typeOf[T].baseClasses.map(typeOf[T].baseType)\n \nprintln(\"*** KMeans instance base classes ***\")\ntypesOf(kmeans).foreach(println)\n \nprintln(\"\\n\\n*** KMeansModel base classes ***\")\ntypesOf(model).foreach(println)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s print the three centroids of our model\n\n// COMMAND ----------\n\ncenters.foreach(println)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Note that our predicted cluster is appended, as a column, to our input `DataFrame`.  Here it would be desirable to see consistency between label and prediction.  These don\u0027t need to be the same number but if label 0 is usually predicted to be cluster 1 that would indicate that our unsupervised learning is naturally grouping the data into species.\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### Using MLlib instead of ML\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC First, convert our `DataFrame` into an `RDD`.\n\n// COMMAND ----------\n\n// Note that .rdd is not necessary, but is here to illustrate that we are working with an RDD\nval irisTwoFeaturesRDD \u003d irisTwoFeatures\n  .rdd\n  .map(r \u003d\u003e (r.getAs[Double](1), r.getAs[Vector](0)))\nirisTwoFeaturesRDD.take(2)\n\n// COMMAND ----------\n\nimport org.apache.spark.sql.Row\n// Or alternatively, we could use a pattern match on the Row case class\nval irisTwoFeaturesRDD \u003d irisTwoFeatures\n  .rdd\n  .map { case Row(feature: Vector, label: Double) \u003d\u003e (label, feature) }\nirisTwoFeaturesRDD.take(2)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Then import MLlib\u0027s `KMeans` as `MLlibKMeans` to differentiate it from `ml.KMeans`\n\n// COMMAND ----------\n\nimport org.apache.spark.mllib.clustering.{KMeans \u003d\u003e MLlibKMeans}\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Finally, let\u0027s build our k-means model.  Here are the relevant [Python](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.clustering.KMeans) and [Scala](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.KMeans) APIs.\n// MAGIC  \n// MAGIC Make sure to set `k` to 3, `maxIterations` to 20, `seed` to 5, and `initializationSteps` to 1.  Also, note that were returned an `RDD` with (label, feature) tuples.  You\u0027ll just need the features, which you can obtain by calling `.values()` on `irisTwoFeaturesRDD`.\n\n// COMMAND ----------\n\n// ANSWER\nimport runtime.ScalaRunTime.stringOf\n \nval mllibKMeans \u003d new MLlibKMeans()\n  .setK(3)\n  .setMaxIterations(20)\n  .setSeed(5)\n  .setInitializationSteps(1)\n  .run(irisTwoFeaturesRDD.values)\n \nprintln(s\"mllib: ${stringOf(mllibKMeans.clusterCenters)}\")\nprintln(s\"ml:    ${stringOf(centers)}\")\n\n// COMMAND ----------\n\n// TEST\nassert(math.round(mllibKMeans.clusterCenters(0)(0) * 1000) \u003d\u003d math.round(centers(0)(0) * 1000),\n       \"Your mllib and ml models don\u0027t match\")\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Now that we have an `mllibKMeans` model how do we generate predictions and compare those to our labels?\n\n// COMMAND ----------\n\nval predictionsRDD \u003d mllibKMeans.predict(irisTwoFeaturesRDD.values)\npredictionsRDD.take(5)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC We\u0027ll use `zip` to combine the feature and prediction RDDs together.  Note that zip assumes that the RDDs have the same number of partitions and that each partition has the same number of elements.  This is true here as our predictions were the result of a `map` operation on the feature RDD.\n\n// COMMAND ----------\n\nval combinedRDD \u003d irisTwoFeaturesRDD.zip(predictionsRDD)\ncombinedRDD.take(5).foreach(println)\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC Let\u0027s compare this to the result from `ml`.\n\n// COMMAND ----------\n\n//display(transformed)\n\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC #### How do the `ml` and `mllib` implementations differ?\n\n// COMMAND ----------\n\n// MAGIC %md\n// MAGIC The `ml` version of k-means is just a wrapper to MLlib\u0027s implementation.  Let\u0027s take a look here:\n// MAGIC [org.apache.spark.ml.clustering.KMeans source](https://github.com/apache/spark/blob/e1e77b22b3b577909a12c3aa898eb53be02267fd/mllib/src/main/scala/org/apache/spark/ml/clustering/KMeans.scala#L192).\n// MAGIC  \n// MAGIC How is $ being used in this function? `Param` [source code](https://github.com/apache/spark/blob/2b574f52d7bf51b1fe2a73086a3735b633e9083f/mllib/src/main/scala/org/apache/spark/ml/param/params.scala#L643) has the answer.\n// MAGIC  \n// MAGIC Which is different than $\u0027s usage for SQL columns where it is a [string interpolator that returns a ColumnName](https://github.com/apache/spark/blob/3d683a139b333456a6bd8801ac5f113d1ac3fd18/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala#L386)\n\n// COMMAND ----------\n\n// So $ followed by a string is treated as a custom string interpolator that creates a ColumnName\nval num \u003d 10\n$\"column$num\"\n",
      "dateUpdated": "Dec 12, 2015 9:59:22 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445940693913_-273584014",
      "id": "20151027-101133_996779038",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.util.MLUtils\nbaseDir: String \u003d /root/pipeline/datasets/misc/\nirisPath: String \u003d /root/pipeline/datasets/misc/iris.scale\nirisRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] \u003d MapPartitionsRDD[1748] at map at MLUtils.scala:112\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/root/pipeline/datasets/misc/iris.scale\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1277)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1272)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:88)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:93)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:95)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:97)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:99)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:101)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:103)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:105)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:107)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:111)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:113)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:115)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:117)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:119)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:121)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:123)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:125)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:127)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:129)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:131)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:133)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:135)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:137)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:139)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:141)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:143)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:145)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:147)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:149)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:151)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:153)\n\tat \u003cinit\u003e(\u003cconsole\u003e:155)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:159)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:655)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:620)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:613)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:276)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:170)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Oct 27, 2015 10:11:33 AM",
      "dateStarted": "Dec 12, 2015 9:59:22 AM",
      "dateFinished": "Dec 12, 2015 9:59:22 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Oct 27, 2015 2:16:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1445940717457_1976858275",
      "id": "20151027-101157_885075371",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Oct 27, 2015 10:11:57 AM",
      "dateStarted": "Oct 27, 2015 2:16:29 PM",
      "dateFinished": "Oct 27, 2015 2:16:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "MLlib/02: KMeans Clustering",
  "id": "2B4QQ3ZE2",
  "angularObjects": {
    "2ARR8UZDJ": [],
    "2AS9P7JSA": [],
    "2AR33ZMZJ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}