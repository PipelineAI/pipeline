{
  "metadata" : {
    "name" : "Tachon Test",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Use Tachyon has the local share tool (WIP)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Context"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "This notebook aims to show how the local tachyon that is started automatically with the notebook can be used in order to cache data OFF_HEAP and can either be:\n* reused in a current notebook\n* or more interestingly here, reused from within **another** notebook"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<span style=\"color:red\">/!\\</span> The second, and hopefully ephemeral, aim of this notebook is to show what is currently possible and asks the **Tachyon experts** what can be the next step to make the cached data reusable easily. \n\nThat's why there is an important <span style=\"font-size:14pt; color: darkgreen;\">Questions</span> section at the end."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**IMPORTANT**: you should have started the notebook server with <span style=\"font-size:18pt;\">enough memory</span> to have tachyon working fine, if <span style=\"font-size:18pt;\">not, please restart</span> the notebook server this way (for `8G` of memory):\n```\nsbt -mem 8000\n```"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Usage"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Running the code can be done is several ways:\n* 1 by 1: using `CTRL+Return`\n* 1 by 1 and going to the next using `SHIFT+Return`\n* all at once from the menu: `Cell` > `Run All`"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Env"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "In the next block we can see the current spark configuration, and specifically the properties:\n* spark.tachyonStore.url\n* spark.tachyonStore.url\n* spark.tachyonStore.folderName"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "sparkContext.getConf.toDebugString",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res2: String = \nspark.app.id=local-1431705813127\nspark.app.name=Notebook\nspark.driver.host=192.168.1.144\nspark.driver.port=39330\nspark.executor.id=<driver>\nspark.fileserver.uri=http://192.168.1.144:56884\nspark.jars=\nspark.master=local[*]\nspark.repl.class.uri=http://192.168.1.144:56703\nspark.tachyonStore.baseDir=/share\nspark.tachyonStore.folderName=spark-e51c8962-1129-4cc3-8f4a-2d631d6e093d\nspark.tachyonStore.url=tachyon://noootsab-beef:18998\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "spark.app.id=local-1431705813127\nspark.app.name=Notebook\nspark.driver.host=192.168.1.144\nspark.driver.port=39330\nspark.executor.id=&lt;driver&gt;\nspark.fileserver.uri=http://192.168.1.144:56884\nspark.jars=\nspark.master=local[*]\nspark.repl.class.uri=http://192.168.1.144:56703\nspark.tachyonStore.baseDir=/share\nspark.tachyonStore.folderName=spark-e51c8962-1129-4cc3-8f4a-2d631d6e093d\nspark.tachyonStore.url=tachyon://noootsab-beef:18998\n <div class='pull-right text-info'><small>349 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Experiment"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Download the data locally (regular csv file on stock prices)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red\"><strong>IMPORTANT:</strong> you will need</p>\n\n* the `wget` tool to download the file\n* a writeable `/tmp` folder where the file will be downloaded by default\n* `174Mb` of free disk"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataUrl = \"https://s3-eu-west-1.amazonaws.com/spark-notebook-data/closes.csv\"\nval dataLocalDowloadPath = \"/tmp/closes.csv\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataUrl: String = https://s3-eu-west-1.amazonaws.com/spark-notebook-data/closes.csv\ndataLocalDowloadPath: String = /tmp/closes.csv\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/closes.csv\n <div class='pull-right text-info'><small>315 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**Recall: this can take some time → downloading from s3 a file of 174M**"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : ":sh wget $dataUrl -O $dataLocalDowloadPath",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataFile = dataLocalDowloadPath",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataFile: String = /tmp/closes.csv\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/closes.csv\n <div class='pull-right text-info'><small>292 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Let's check the number of lines in the file"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":sh wc -l $dataFile",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 1 feature warning(s); re-run with -feature for details\nimport sys.process._\nres4: scala.xml.Elem = \n<pre>8655036 /tmp/closes.csv\n</pre>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<pre>8655036 /tmp/closes.csv\n</pre>\n <div class='pull-right text-info'><small>472 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "markdown",
    "source" : "### Reading data and cache it as usual"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val rdd = sparkContext.textFile(dataFile).cache",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd: org.apache.spark.rdd.RDD[String] = /tmp/closes.csv MapPartitionsRDD[1] at textFile at <console>:41\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/closes.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:41\n <div class='pull-right text-info'><small>776 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Dummy computation to force the cache"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "rdd.flatMap(_.filter(_ == '0')).map(_.toString.toInt).sum",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res5: Double = 0.0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "0.0\n <div class='pull-right text-info'><small>6 seconds 372 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "At this stage, we can check [http://localhost:4040/storage](http://localhost:4040/storage) and we'll see that (depending on the available memory) a fraction if not the whole file has been cached."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "markdown",
    "source" : "### Now we use Tachyon to cache"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.storage.StorageLevel._\nval rdd2 = sparkContext.textFile(dataFile).persist(OFF_HEAP)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.storage.StorageLevel._\nrdd2: org.apache.spark.rdd.RDD[String] = /tmp/closes.csv MapPartitionsRDD[6] at textFile at <console>:42\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/closes.csv MapPartitionsRDD[6] at textFile at &lt;console&gt;:42\n <div class='pull-right text-info'><small>260 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Dummy computation to force the cache"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "rdd2.flatMap(_.filter(_ == '0')).map(_.toString.toInt).sum",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res6: Double = 0.0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "0.0\n <div class='pull-right text-info'><small>4 seconds 272 milliseconds</small></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Now, we can check [http://localhost:4040/storage](http://localhost:4040/storage) **again** and <span style=\"color:darkred\">Yeepee</span> the data has been cached in tachyon (_see `size in tachyon`_ column)."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Let's explore the Tachyon's content"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "import tachyon.client.TachyonFS\nval tachyonUrl = sparkContext.getConf.get(\"spark.tachyonStore.url\")\nval shareDir = sparkContext.getConf.get(\"spark.tachyonStore.baseDir\")\nval tachyonClient = TachyonFS.get(tachyonUrl)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<strong>Here is the content of the `share` dir of Spark</strong>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "import scala.collection.JavaConverters._\nval shared = tachyonClient.ls(shareDir, false).asScala",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val currentCache = shared(1)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<strong>AFAICT, spark will use `<driver>` as the cache folder</strong>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val currentFiles = tachyonClient.ls(currentCache + \"/<driver>\", false).asScala",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##### Here is the RDD that has been cached! → _ala_ hadoop with each partition written in its own folder and file part"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val thisRdd = currentFiles(1)\nval whatsInCurrentThisRdd = tachyonClient.ls(thisRdd, true).asScala.map(_.drop(thisRdd.length))",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Above we see the cached folders and partitions"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<h2 id=\"questions\" name=\"questions\">QUESTIONS :-D</h2>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red;\">How to not do '/*'?</p>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "So far, to load the whole data set back from the tachyon cache, we do\n```\nval tachyonThisFileRdd = sparkContext.textFile(tachyonUrl + thisRdd + \"/*\")\n```\n\nWhich is not exaclty what we should do I guess, I mean the `/*` shouldn't be required?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val tachyonThisFileRdd = sparkContext.textFile(tachyonUrl + thisRdd + \"/*\")",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red;\">The following will get the whole file as the result</p>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "//  tachyonRdd.take(1).head",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<p style=\"color:red\">How to have the returned `RDD` having elements being a line rather than the whole file?</p>"
  } ],
  "nbformat" : 4
}