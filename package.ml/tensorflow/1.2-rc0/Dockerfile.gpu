FROM fluxcapacitor/package-gpu-cuda8-16.04:master

WORKDIR /root

ENV \
  TERM=xterm


###################
# Setup OpenJDK 1.8
###################
RUN \
  apt-get install -y software-properties-common \
  && add-apt-repository -y ppa:openjdk-r/ppa \
  && apt-get update \
  && apt-get install -y --no-install-recommends openjdk-8-jdk openjdk-8-jre-headless \
  && apt-get install -y apt-transport-https \
  && apt-get install -y wget \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

ENV \
  JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/


###################
# Setup Bazel 0.4.5
###################
ENV \
  BAZEL_VERSION=0.4.5 

# Running bazel inside a `docker build` command causes trouble, cf:
#   https://github.com/bazelbuild/bazel/issues/134
# The easiest solution is to set up a bazelrc file forcing --batch.
RUN echo "startup --batch" >>/etc/bazel.bazelrc

# Similarly, we need to workaround sandboxing issues:
#   https://github.com/bazelbuild/bazel/issues/418
RUN echo "build --spawn_strategy=standalone --genrule_strategy=standalone" \
    >>/etc/bazel.bazelrc

# Install the most recent bazel release.
RUN mkdir /root/bazel && \
    cd /root/bazel && \
    curl -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \
    chmod a+x bazel-*.sh && \
    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \
    rm -f bazel-$BAZEL_VERSION-installer-linux-x86_64.sh


##########################
# Build TensorFlow Serving
##########################
ENV \
  TENSORFLOW_SERVING_VERSION=0.5.1 \
  TENSORFLOW_SERVING_HOME=/root/serving 

# Required by TensorFlow Serving
# Inspired by the following:
#   https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0/
#   https://github.com/tensorflow/serving/blob/1697e743b5a001d0f79274d94d2d8d570a09865b/tensorflow_serving/tools/docker/Dockerfile.devel-gpu
RUN \
  apt-get update \
  && apt-get install -y \
        build-essential \
        curl \
        libcurl3-dev \
        git \
        libfreetype6-dev \
        libpng12-dev \
        libzmq3-dev \
        pkg-config \
        python-dev \
        python3-dev \
        python-numpy \
        python3-numpy \
        python-six \
        python3-six \
        python-wheel \
        python3-wheel \
        python-pip \
        python3-pip \
        software-properties-common \
        swig \
        zip \
        zlib1g-dev \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

# Install Anaconda with Python3 
RUN wget -q https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh -O /tmp/miniconda.sh  && \
    echo '874dbb0d3c7ec665adf7231bbb575ab2 */tmp/miniconda.sh' | md5sum -c - && \
    bash /tmp/miniconda.sh -f -b -p /opt/conda && \
    /opt/conda/bin/conda install --yes python=3.5 sqlalchemy tornado jinja2 traitlets requests pip && \
    /opt/conda/bin/pip install --upgrade pip && \
    rm /tmp/miniconda.sh

ENV \
  PATH=/opt/conda/bin:$PATH

RUN \
  pip install --upgrade pip

RUN \
  pip install grpcio

RUN \
  conda install --yes openblas scikit-learn numpy scipy matplotlib pandas seaborn

# Clone Tensorflow Serving and the Tensorflow Submodule
RUN \
 cd ~ \
 && git clone -b master --recurse-submodules https://github.com/tensorflow/serving.git \
 && cd $TENSORFLOW_SERVING_HOME \
 && git reset --hard 0aca2e0 

ENV TF_NEED_CUDA=1
ENV TF_NEED_GCP=0
ENV TF_NEED_JEMALLOC=1
ENV TF_NEED_HDFS=1
ENV TF_NEED_OPENCL=0
ENV TF_ENABLE_XLA=0
ENV TF_CUDA_VERSION=8.0
ENV TF_CUDNN_VERSION=5
ENV CUDA_PATH="/usr/local/cuda"
ENV CUDA_TOOLKIT_PATH=$CUDA_PATH
ENV CUDNN_INSTALL_PATH=$CUDA_PATH
ENV PYTHON_BIN_PATH=/opt/conda/bin/python
ENV PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages
ENV CI_BUILD_PYTHON=$PYTHON_BIN_PATH
ENV CC_OPT_FLAGS="-march=native"
ENV TF_CUDA_COMPUTE_CAPABILITIES=3.7
####################
###      AWS     ###
##  P2 Instances  ##
# Tesla K-80 (3.7) #
#                  #
##  G2 Instances  ##
# GRID K520 (3.5)  #
#                  #
###  Google GCP  ###   
# Tesla K-80 (3.7) #
####################

RUN \
  cd $TENSORFLOW_SERVING_HOME/tensorflow \
  && tensorflow/tools/ci_build/builds/configured GPU

RUN \
  cd $TENSORFLOW_SERVING_HOME \
  # Remove NCCL since it isn't building properly
  && sed -i.bak '/nccl/d' tensorflow/tensorflow/contrib/BUILD \
  && bazel build -c opt --config=cuda \
      --verbose_failures \
      --spawn_strategy=standalone --genrule_strategy=standalone \
      --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 \
      --crosstool_top=@local_config_cuda//crosstool:toolchain \
      tensorflow_serving/model_servers:tensorflow_model_server \
  && chmod a+x bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server \
  && cp bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server /usr/local/bin/ \
  && bazel clean --expunge


##################
# Build TensorFlow
##################
ENV \
  TENSORFLOW_HOME=/root/tensorflow

# Need this inside Docker for nvidia-docker build step HACK
ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH
RUN \
  mkdir -p /usr/local/nvidia/lib64/ \
  && cd /usr/local/nvidia/lib64/ \
  && ln -s /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libcuda.so libcuda.so.1
RUN \
  ldconfig /usr/local/cuda/lib64

RUN \
 git clone -b r1.2 --recurse-submodules https://github.com/tensorflow/tensorflow.git \
 && cd $TENSORFLOW_HOME \
 && git reset --hard c9dd88f

ENV TF_NEED_CUDA=1
ENV TF_NEED_GCP=0
ENV TF_NEED_JEMALLOC=1
ENV TF_NEED_HDFS=1
ENV TF_NEED_OPENCL=0
ENV TF_ENABLE_XLA=1
ENV TF_CUDA_VERSION=8.0
ENV TF_CUDNN_VERSION=5
ENV CUDA_PATH="/usr/local/cuda"
ENV CUDA_TOOLKIT_PATH=$CUDA_PATH
ENV CUDNN_INSTALL_PATH=$CUDA_PATH
ENV PYTHON_BIN_PATH=/opt/conda/bin/python
ENV PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages
ENV CI_BUILD_PYTHON=$PYTHON_BIN_PATH
ENV CC_OPT_FLAGS="-march=native"
ENV TF_CUDA_COMPUTE_CAPABILITIES=3.7
####################
###      AWS     ###
##  P2 Instances  ##
# Tesla K-80 (3.7) #
#                  #
##  G2 Instances  ##
# GRID K520 (3.5)  #
#                  #
###  Google GCP  ###
# Tesla K-80 (3.7) #
####################

# This build command is inspired by the following resources:
#   https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh
#   https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu
#   http://ci.tensorflow.org/job/tensorflow-master-linux-xla/104/consoleText
#   https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build

RUN \
  cd $TENSORFLOW_HOME \
  && yes "" | ./configure

ENV \
  CPU_COUNT=8 \
  GPU_COUNT=1 

RUN \
  cd $TENSORFLOW_HOME \
  && sed -i.bak 's/EXPECT_FALSE(isnan(value));/EXPECT_FALSE(0);/g' tensorflow/core/kernels/mfcc_test.cc \
  && bazel build -c opt --config=cuda \
     --jobs=${CPU_COUNT} \
     --verbose_failures \
     --test_timeout 300,450,1200,3600 \
     --test_output=errors \
     --local_test_jobs=${GPU_COUNT} \
     --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 \
     --cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" \
     --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute -- \
     //tensorflow/...

RUN \
  cd $TENSORFLOW_HOME \
  && bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip \
  && pip --no-cache-dir install --ignore-installed --upgrade /tmp/pip/tensorflow-*.whl

# Clean up pip wheel and Bazel cache when done.
#RUN \
#    pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl && \
#    rm -rf /tmp/pip && \
#    rm -rf /root/.cache

ENV \
  TF_CPP_MIN_LOG_LEVEL=0 \
  TF_XLA_FLAGS=--xla_generate_hlo_graph=.*

## Cleanup for nvidia-docker build step HACK
RUN \
  rm /usr/local/nvidia/lib64/libcuda.so.1
