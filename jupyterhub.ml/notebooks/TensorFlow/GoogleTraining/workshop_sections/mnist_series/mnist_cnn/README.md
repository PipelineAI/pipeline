
# Creating a Custom MNIST Estimator

  - [A CNN version of MNIST using TensorFlow's "low-level" API](#a-cnn-version-of-mnist-using-tensorflows-low-level-api)
  - [Exercise 1: MNIST-CNN using a Custom Estimator](#exercise-1-mnist-cnn-using-a-custom-estimator)
    - [Look at the output in TensorBoard](#look-at-the-output-in-tensorboard)
  - [Exercise 2: convert the "low-level API" Estimator model to one that uses *layers*](#exercise-2-convert-the-low-level-api-estimator-model-to-one-that-uses-layers)
  - [Compare training runs](#compare-training-runs)

In a previous lab, we used `tf.contrib.tflearn.DNNClassifier` to easily build our MNIST model.
However, it turns out that a model that uses *convolutions* performs better on this task.  There is currently no "pre-baked" class that we can use (instead of `DNNClassifier`) for the CNN model.

So instead, we can build a *custom* [Estimator](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.learn.html#Estimator).  We'll still need to define the model specifics, but we can leverage the Estimator's support for running training loops, model evaluations, and model predictions; as well as model checkpointing during training and generating info for TensorBoard.

This lab has two stages.  In the first stage, we'll see how we can take a pre-existing model graph definition and essentially plop it into an Estimator, making our new version simpler.

In the second stage of the lab, we'll look at how we can use [layers](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html#layers-contrib) to make our model specification more concise and easier to build and understand.

## A CNN version of MNIST using TensorFlow's "low-level" API

The [`mnist_cnn.py`](mnist_cnn.py) script follows the [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html#deep-mnist-for-experts) tutorial on the [tensorflow.org](http://tensorflow.org) site.

We can use this code, and its model, as a starting point for the Estimators that we're going to build.

## Exercise 1: MNIST-CNN using a Custom Estimator

In this first exercise, we'll look at how we can create a custom [Estimator](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.learn.html#Estimator) based the [`mnist_cnn.py`](mnist_cnn.py) model graph, using TensorFlow's high-level `contrib.tflearn` API.

As with the "canned" Estimators, like the [DNNClassifier](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.learn.html#DNNClassifier) used in one of the [other MNIST labs](../02_README_mnist_tflearn.md), the `Estimator`provides support for the `fit()`, `evaluate()`, and `predict()` functions so that you don't have to write a training loop, manage model checkpointing, etc., yourself.

Start with this 'skeleton' file: [`mnist_cnn_estimator_skeleton.py`](mnist_cnn_estimator_skeleton.py).
Look for the "YOUR CODE HERE" notes.

1. First, fill in the `model_fn` function.

2. Then, in the `run_classifier` function,  create a `tf.contrib.learn.Estimator` called `cnn`, and call its `fit()` method to train it.

[This example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/estimators/abalone.py) in the TensorFlow repo might be useful.

If you get stuck, you can take a look at `mnist_cnn_estimator.py`, but try not to look too soon.

Run your finished script like this (substituting your file name):

```sh
$ python mnist_cnn_estimator.py
```

### Look at the output in TensorBoard

You can take a look at its output in TensorBoard as follows, replacing `<your-model-dir>` with the timestamped directory generated by the script.

```sh
$ tensorboard --logdir=/tmp/tfmodels/mnist_estimator/<your-model-dir>
```

For fun, take a look at the model graph in TensorBoard.

<a href="https://storage.googleapis.com/oscon-tf-workshop-materials/images/mnist_cnn_estim_graph.png" target="_blank"><img src="https://storage.googleapis.com/oscon-tf-workshop-materials/images/mnist_cnn_estim_graph.png" width="500"/></a>

Next, we'll make this code even simpler by using *layers*.

## Exercise 2: convert the "low-level API" Estimator model to one that uses *layers*

Start with the result of Exercise 1, or with [`mnist_cnn_estimator.py`](mnist_cnn_estimator.py), which wraps the model
graph from `mnist_cnn.py` in an `Estimator`.

Modify this script to replace the model definition code-- the code in the `model_fn` function-- with simpler code that defines the model graph using the [tf.contrib.layers](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html#layers-contrib) library.  Browse this link to see what is available to you. You can take a look in the tensorflow repo [here](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/contrib/layers/python/layers/layers.py) as well.

If you get stuck, you can take a look at `mnist_cnn_estim_layers.py`, but try not to look too soon.

## Compare training runs

When you're done, try comparing the output of both training runs— that of `mnist_cnn_estimator.py` and your new layers version— in TensorBoard.  You can do this by pointing TensorBoard to the parent directory:

```sh
$ tensorboard --logdir=/tmp/tfmodels/mnist_estimator
```

Take a look again at the TensorBoard info.  (You'll notice that the graph nodes are being packaged up a bit more hierarchically this time, due to the use of the 'layers' methods.)

<a href="https://storage.googleapis.com/oscon-tf-workshop-materials/images/mnist_cnn_layers_graph.png" target="_blank"><img src="https://storage.googleapis.com/oscon-tf-workshop-materials/images/mnist_cnn_layers_graph.png" width="500"/></a>

Note: The [transfer learning](../../transfer_learning/README.md) and [word2vec](../../word2vec/README.md) labs show additional examples of building and using custom Estimators.
