{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "This notebook is similar in functionality to [this python script](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/mnist_tflearn.py), and is used with [this README](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/02_README_mnist_tflearn.md).  It shows how to use TensorFlow's high-level apis, in `contrib.tflearn`, to easily build a classifier with multiple hidden layers.\n",
    "\n",
    "First, do some imports and set some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "DATA_DIR = \"MNIST_data\"\n",
    "MODEL_DIR = os.path.join(\"/tmp/tfmodels/mnist_tflearn\",\n",
    "                         str(int(time.time())))\n",
    "NUM_STEPS = 25000\n",
    "# read in data, downloading first as necessary\n",
    "DATA_SETS = input_data.read_data_sets(DATA_DIR)\n",
    "\n",
    "# comment out for less info during the training runs.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add a function that defines a `DNNClassifier`, and runs its `fit()` method, which will train the model. Note that we didn't need to explicitly define a model graph or a training loop ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_and_run_dnn_classifier():\n",
    "    \"\"\"Run a DNN classifier.\"\"\"\n",
    "    feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(\n",
    "        DATA_SETS.train.images)\n",
    "    classifier = tf.contrib.learn.DNNClassifier(\n",
    "        feature_columns=feature_columns, n_classes=10,\n",
    "        hidden_units=[128, 32],\n",
    "        # After you've done a training run with optimizer learning rate 0.1,\n",
    "        # change it to 0.5 and run the training again.  Use TensorBoard to take\n",
    "        # a look at the difference.  You can see both runs by pointing it to the\n",
    "        # parent model directory, which by default is:\n",
    "        #   tensorboard --logdir=/tmp/tfmodels/mnist_tflearn\n",
    "        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.1),\n",
    "        model_dir=MODEL_DIR\n",
    "        )\n",
    "    classifier.fit(DATA_SETS.train.images,\n",
    "                   DATA_SETS.train.labels.astype(numpy.int64),\n",
    "                   batch_size=100, max_steps=NUM_STEPS)\n",
    "    print(\"Finished running the training via the fit() method\")\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, add a function that runs the classifier's `evaluate()` method.  To do this, it loads the most recent checkpointed model info available. The model checkpoint(s) will be generated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_dnn_classifier(classifier):\n",
    "    # Evaluate classifier accuracy.\n",
    "    accuracy_score = classifier.evaluate(\n",
    "        DATA_SETS.test.images,\n",
    "        DATA_SETS.test.labels.astype(numpy.int64))['accuracy']\n",
    "    print('DNN Classifier Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, call the function that defines and trains the model. With the default number of steps (25000), it will take a few minutes. Wait for the \"finished running...\" message to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Running DNN classifier...\")\n",
    "classifier = define_and_run_dnn_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've trained the model, evaluate how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_dnn_classifier(classifier)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
