{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "This notebook is similar to [this python script](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/mnist_onehlayer.py), and uses [this README](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/03_README_mnist_layers.md).\n",
    "It implements a \"one hidden layer\" version of MNIST. \n",
    "As part of this lab, you'll add another hidden layer (and make a few other small modifications).  You can make your edits in the notebook if you like, or edit the corresponding python script.\n",
    "\n",
    "Start with some imports and variable definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "\n",
    "# Define some constants.\n",
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "# Batch size. Must be evenly dividable by dataset sizes.\n",
    "BATCH_SIZE = 100\n",
    "EVAL_BATCH_SIZE = 3\n",
    "# Number of units in hidden layers.\n",
    "HIDDEN1_UNITS = 128\n",
    "\n",
    "NUM_STEPS = 25000\n",
    "DATA_DIR = \"MNIST_data\"\n",
    "MODEL_DIR = default=os.path.join(\n",
    "    \"/tmp/tfmodels/mnist_onehlayer\",\n",
    "    str(int(time.time())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a function that builds the core of the model graph.  We'll see below, when this function is called, that the 'images' arg passed to this function is the images input placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build inference graph.\n",
    "def mnist_inference(images, hidden1_units):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "    Args:\n",
    "        images: Images placeholder.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "    Returns:\n",
    "        logits: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, NUM_CLASSES],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden1, weights) + biases\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a function that builds on the inference graph above in order to create a training graph. We define a loss function, create an optimizer, and create a 'train_op' that tells the optimizer to apply the gradients that minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build training graph.\n",
    "def mnist_training(logits, labels, learning_rate):\n",
    "    \"\"\"Build the training graph.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [BATCH_SIZE, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [BATCH_SIZE], with values in the\n",
    "          range [0, NUM_CLASSES).\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates loss.\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    \n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll put it all together. We process the input data and generate the necessary placeholders.\n",
    "We'll also add ops to the graph for calculating accuracy.  \n",
    "We'll also add support for generating \"summary\" info used by TensorBoard.\n",
    "\n",
    "Then, we create a session in which to run a training loop. \n",
    "After training, the model is checkpointed to a file.\n",
    "\n",
    "Finally, we show that we can load a checkpointed model file into a new session, and run a prediction based on the checkpointed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_train_and_eval():\n",
    "    \"\"\"Build the full graph for feeding inputs, training, and\n",
    "    saving checkpoints.  Run the training. Then, load the saved graph and\n",
    "    run some predictions.\"\"\"\n",
    "\n",
    "    # Get input data: get the sets of images and labels for training,\n",
    "    # validation, and test on MNIST.\n",
    "    data_sets = read_data_sets(DATA_DIR, False)\n",
    "\n",
    "    mnist_graph = tf.Graph()\n",
    "    with mnist_graph.as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder = tf.placeholder(tf.float32)\n",
    "        labels_placeholder = tf.placeholder(tf.int32)\n",
    "        tf.add_to_collection(\"images\", images_placeholder)  # Remember this Op.\n",
    "        tf.add_to_collection(\"labels\", labels_placeholder)  # Remember this Op.\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = mnist_inference(images_placeholder,\n",
    "                                 HIDDEN1_UNITS)\n",
    "        tf.add_to_collection(\"logits\", logits)  # Remember this Op.\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op, loss = mnist_training(\n",
    "            logits, labels_placeholder, 0.01)\n",
    "\n",
    "        # prediction accuracy\n",
    "        _, indices_op = tf.nn.top_k(logits)\n",
    "        flattened = tf.reshape(indices_op, [-1])\n",
    "        correct_prediction = tf.cast(\n",
    "            tf.equal(labels_placeholder, flattened), tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "        # Define info to be used by the SummaryWriter. This will let\n",
    "        # TensorBoard plot values during the training process.\n",
    "        loss_summary = tf.scalar_summary(\"loss\", loss)\n",
    "        train_summary_op = tf.merge_summary([loss_summary])\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Create a summary writer.\n",
    "        print(\"Writing Summaries to %s\" % MODEL_DIR)\n",
    "        train_summary_writer = tf.train.SummaryWriter(MODEL_DIR)\n",
    "\n",
    "    # Run training for MAX_STEPS and save checkpoint at the end.\n",
    "    with tf.Session(graph=mnist_graph) as sess:\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start the training loop.\n",
    "        print(\"Starting training...\")\n",
    "        for step in xrange(NUM_STEPS):\n",
    "            # Read a batch of images and labels.\n",
    "            images_feed, labels_feed = data_sets.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "            _, loss_value, tsummary, acc = sess.run(\n",
    "                [train_op, loss, train_summary_op, accuracy],\n",
    "                feed_dict={images_placeholder: images_feed,\n",
    "                           labels_placeholder: labels_feed})\n",
    "            if step % 100 == 0:\n",
    "                # Write summary info\n",
    "                train_summary_writer.add_summary(tsummary, step)\n",
    "            if step % 1000 == 0:\n",
    "                # Print loss/accuracy info\n",
    "                print('----Step %d: loss = %.4f' % (step, loss_value))\n",
    "                print(\"accuracy: %s\" % acc)\n",
    "\n",
    "        print(\"\\nFinished training. Writing checkpoint file.\")\n",
    "        checkpoint_file = os.path.join(MODEL_DIR, 'checkpoint')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        _, loss_value = sess.run(\n",
    "            [train_op, loss],\n",
    "            feed_dict={images_placeholder: data_sets.test.images,\n",
    "                       labels_placeholder: data_sets.test.labels})\n",
    "        print(\"Test set loss: %s\" % loss_value)\n",
    "\n",
    "    # Run evaluation based on the saved checkpoint.\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        checkpoint_file = tf.train.latest_checkpoint(MODEL_DIR)\n",
    "        print(\"\\nRunning predictions based on saved checkpoint.\")\n",
    "        print(\"checkpoint file: {}\".format(checkpoint_file))\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Retrieve the Ops we 'remembered'.\n",
    "        logits = tf.get_collection(\"logits\")[0]\n",
    "        images_placeholder = tf.get_collection(\"images\")[0]\n",
    "        labels_placeholder = tf.get_collection(\"labels\")[0]\n",
    "\n",
    "        # Add an Op that chooses the top k predictions.\n",
    "        eval_op = tf.nn.top_k(logits)\n",
    "\n",
    "        # Run evaluation.\n",
    "        images_feed, labels_feed = data_sets.validation.next_batch(\n",
    "            EVAL_BATCH_SIZE)\n",
    "        prediction = sess.run(eval_op,\n",
    "                              feed_dict={images_placeholder: images_feed,\n",
    "                                         labels_placeholder: labels_feed})\n",
    "        for i in range(len(labels_feed)):\n",
    "            print(\"Ground truth: %d\\nPrediction: %d\" %\n",
    "                  (labels_feed[i], prediction.indices[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payoff for all those function definitions... now we'll run `model_train_and_eval()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_train_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model did okay... but not great.  Next we'll try adding another hidden layer.\n",
    "\n",
    "Contrast this code with that in the [previous lab](https://github.com/amygdala/tensorflow-workshop/blob/master/workshop_sections/mnist_series/02_README_mnist_tflearn.md), which used TensorFlow's high-level APIs. You can probably see how it's much easier to use those high-level \"pre-canned\" Estimators where appropriate."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
